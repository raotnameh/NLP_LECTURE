{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os, glob, pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re, string\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, OrderedDict\n",
    "import utils.score as evaluate\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.zip  sentiment_test.csv  sentiment_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/3a/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "\n",
    "def prepr(txt):\n",
    "    dummy = []\n",
    "    for i in txt.split():\n",
    "        if '@' in i: dummy.append('@someuser')\n",
    "        elif '#' in  i: dummy.append('#someuser')\n",
    "        elif i in stopwords: continue\n",
    "        else: dummy.append(remove_punctuation(i))\n",
    "    \n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes_():\n",
    "    def __init__(self,train, test):\n",
    "        self.train_x = [prepr(i.lower()) for i in tqdm(train['x'])]\n",
    "        self.train_y = [i for i in tqdm(train['y'])]\n",
    "        \n",
    "        if test:\n",
    "            self.test_x = [prepr(i.lower()) for i in tqdm(test['x'])]\n",
    "            self.test_y = [i for i in tqdm(test['y'])]\n",
    "\n",
    "            self.data_x, self.data_y = self.train_x + self.test_x, self.train_y + self.test_y\n",
    "        \n",
    "        else: \n",
    "            self.test_x = None\n",
    "            self.data_x = self.train_x\n",
    "            self.data_y = self.train_y \n",
    "        \n",
    "        self.vocab = sorted([i for i in set([j for i in tqdm(self.data_x) for j in i])])\n",
    "        \n",
    "    def fit(self):\n",
    "        \n",
    "        dummy = Counter(self.train_y)\n",
    "        prior = {i:dummy[i]/len(self.train_y) for i in set(dummy)}\n",
    "        self.labels = prior.keys()\n",
    "\n",
    "        matrix = {i:np.zeros(len(self.vocab)) for i in self.labels}\n",
    "        self.vti = {i:d for d,i in enumerate(self.vocab)}\n",
    "        \n",
    "        dummy = {i:len(self.vocab) for i in self.labels}\n",
    "        for i in tqdm(range(len(self.train_y))):\n",
    "            count = Counter(self.train_x[i])\n",
    "            dummy[self.train_y[i]] += sum(count.values())\n",
    "            \n",
    "            for k,v in count.items(): \n",
    "                matrix[self.train_y[i]][self.vti[k]]+= v\n",
    "        \n",
    "        for k,v in matrix.items():\n",
    "            matrix[k] += 1\n",
    "            matrix[k]/= dummy[k]\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.matrix = matrix\n",
    "        self.dummy = dummy\n",
    "        \n",
    "        return prior, matrix\n",
    "        \n",
    "    def predict(self,test_x,pred=True):\n",
    "        \n",
    "        test_x = [' '.join(prepr(i.lower())) for i in test_x]\n",
    "        pred_y = []\n",
    "\n",
    "        for i in tqdm(test_x):\n",
    "            label = {}\n",
    "            for j in self.labels:\n",
    "                prob = self.prior[j]\n",
    "                for w in i.split():\n",
    "                    if w in self.vti.keys(): prob *= self.matrix[j][self.vti[w]]\n",
    "                    else: prob *= 1/(len(self.vocab) + self.dummy[j])\n",
    "                label[j] = prob\n",
    "#             pred_y.append(label)  \n",
    "            a = sum(label.values())\n",
    "            pred_y.append({k:v/a for k,v in label.items()})\n",
    "        if pred: return [sorted(i.items(), key=lambda x:x[-1], reverse=True)[0][0] for i in pred_y]\n",
    "        else: return pred_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "\n",
    "train_3a = pd.read_csv('data/3a/sentiment_train.csv') # 11.2 lakhs\n",
    "test_3a = pd.read_csv('data/3a/sentiment_test.csv') # 4.8 lakhs\n",
    "\n",
    "dummy = False # Dummy data for testing\n",
    "\n",
    "if not dummy:\n",
    "    train = {\n",
    "                'x': train_3a['5'],\n",
    "                'y': train_3a['0']\n",
    "    }\n",
    "\n",
    "    test = {\n",
    "                'x': test_3a['5'],\n",
    "                'y': [i for i in test_3a['0']]\n",
    "    }\n",
    "\n",
    "else:\n",
    "    train = {\n",
    "                'x': ['chinese beijing chinese', 'chinese chinese shanghai', 'chinese macao','tokyo japan chinese'],\n",
    "                'y': [1,1,1,2]\n",
    "    }\n",
    "\n",
    "    test = {\n",
    "                'x': ['chinese chinese chinese tokyo japan', 'fuck this shit japan japan tokyo'],\n",
    "                'y': [1,2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naive_bayes = naive_bayes_(train,None)\n",
    "prior, matrix = naive_bayes.fit()\n",
    "\n",
    "pred = naive_bayes.predict(test['x'], True)\n",
    "assert len(pred) == len(test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.evaluate(test['y'], pred, naive_bayes.labels)\n",
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(test['y'], pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "\n",
    "header_list = ['id', 'tweet', 'emo', 'intent']\n",
    "\n",
    "df_train = pd.read_table('data/3b/train/out.txt', names=header_list)\n",
    "train_x = [i for i in df_train['tweet']]\n",
    "train_y = [i for i in df_train['emo']]\n",
    "\n",
    "df_test = pd.read_table('data/3b/test/out.txt', names=header_list)\n",
    "test_x = [i for i in df_test['tweet']]\n",
    "test_y = [i for i in df_test['emo']]\n",
    "\n",
    "train = {\n",
    "                'x': train_x,\n",
    "                'y': train_y\n",
    "}\n",
    "\n",
    "test = {\n",
    "            'x': test_x,\n",
    "            'y': [i for i in test_y]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naive_bayes = naive_bayes_(train,None)\n",
    "prior, matrix = naive_bayes.fit()\n",
    "\n",
    "pred = naive_bayes.predict(test['x'], True)\n",
    "assert len(pred) == len(test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.evaluate(test['y'], pred, naive_bayes.labels)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(test['y'], pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = naive_bayes.predict(test['x'], False)\n",
    "pred = [sorted(i.items(), key=lambda x:x[-1], reverse=True)[0] for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_a, p_a = '', ''\n",
    "for i in range(len(test_x)):\n",
    "    t_a += f\"{i}\\t{test_x[i]}\\t{df_test['emo'][i]}\\t{df_test['intent'][i]}\\n\"\n",
    "    p_a += f\"{i}\\t{test_x[i]}\\t{pred[i][0]}\\t{pred[i][1]}\\n\"\n",
    "#     print(t_a,p_a)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_(path, file):\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_('true.txt', t_a)\n",
    "save_('pred.txt', p_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between pred.txt and true.txt:\t-0.014455233629579792\r\n",
      "Spearman correlation between pred.txt and true.txt:\t0.017891406897644083\r\n",
      "Pearson correlation for gold scores in range 0.5-1 between pred.txt and true.txt:\t0.025038320143194705\r\n",
      "Spearman correlation for gold scores in range 0.5-1 between pred.txt and true.txt:\t0.03419810852157647\r\n",
      "\r\n",
      "Average Pearson correlation:\t-0.014455233629579792\r\n",
      "Average Spearman correlation:\t0.017891406897644083\r\n",
      "Average Pearson correlation for gold scores in range 0.5-1:\t0.025038320143194705\r\n",
      "Average Spearman correlationfor gold scores in range 0.5-1:\t0.03419810852157647\r\n"
     ]
    }
   ],
   "source": [
    "!python2 eval.py 1 pred.txt true.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#import IRIS dataset\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "features = iris.feature_names\n",
    "target = iris.target_names\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# spliting data in test and train set keeping 70% data in train set and 30% data in test set. \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decions tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro prec and recall -- 0.9666666666666667 0.9791666666666666\n",
      " Macro F1 score: 0.972876516773733\n",
      "Accuracy: 0.9736842105263158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conf_labels': ['0', '1', '2'],\n",
       " 'conf_matrix': [[13, 0, 0], [0, 15, 1], [0, 0, 9]],\n",
       " 'macro_prec_and_rec': [0.9666666666666667, 0.9791666666666666],\n",
       " 'macro_f1': 0.972876516773733,\n",
       " 'accuracy': 0.9736842105263158}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DT Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Lets fit the data into classifier \n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = dt.predict(X_test)\n",
    "evaluate.evaluate(y_test,y_pred,set(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVC on training set: 98.21\n",
      "Test\n",
      "macro prec and recall -- 0.9666666666666667 0.9791666666666666\n",
      " Macro F1 score: 0.972876516773733\n",
      "Accuracy: 0.9736842105263158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conf_labels': ['0', '1', '2'],\n",
       " 'conf_matrix': [[13, 0, 0], [0, 15, 1], [0, 0, 9]],\n",
       " 'macro_prec_and_rec': [0.9666666666666667, 0.9791666666666666],\n",
       " 'macro_f1': 0.972876516773733,\n",
       " 'accuracy': 0.9736842105263158}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVC Classifier\n",
    "clf_SVC = SVC(C=100.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n",
    "          probability=False, tol=0.001, cache_size=200, class_weight=None, \n",
    "          verbose=0, max_iter=-1, decision_function_shape=\"ovr\", random_state = 0)\n",
    "\n",
    "# Fitting training data\n",
    "clf_SVC.fit(X_train,y_train)\n",
    "\n",
    "# predicting accuracies\n",
    "print('Accuracy of SVC on training set: {:.2f}'.format(clf_SVC.score(X_train, y_train) * 100))\n",
    "\n",
    "# predictions\n",
    "y_pred = clf_SVC.predict(X_test)\n",
    "print('Test')\n",
    "evaluate.evaluate(y_test,y_pred,set(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.16374727\n",
      "Iteration 2, loss = 1.15851323\n",
      "Iteration 3, loss = 1.15304095\n",
      "Iteration 4, loss = 1.14749201\n",
      "Iteration 5, loss = 1.14198799\n",
      "Iteration 6, loss = 1.13633537\n",
      "Iteration 7, loss = 1.13055165\n",
      "Iteration 8, loss = 1.12466765\n",
      "Iteration 9, loss = 1.11903797\n",
      "Iteration 10, loss = 1.11342383\n",
      "Iteration 11, loss = 1.10770560\n",
      "Iteration 12, loss = 1.10229156\n",
      "Iteration 13, loss = 1.09693001\n",
      "Iteration 14, loss = 1.09176170\n",
      "Iteration 15, loss = 1.08671844\n",
      "Iteration 16, loss = 1.08199581\n",
      "Iteration 17, loss = 1.07788282\n",
      "Iteration 18, loss = 1.07379261\n",
      "Iteration 19, loss = 1.06977464\n",
      "Iteration 20, loss = 1.06591248\n",
      "Test\n",
      "macro prec and recall -- 0.22875816993464052 0.38425925925925924\n",
      " Macro F1 score: 0.2867861197492472\n",
      "Accuracy: 0.34210526315789475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conf_labels': ['0', '1', '2'],\n",
       " 'conf_matrix': [[0, 9, 4], [0, 6, 10], [0, 2, 7]],\n",
       " 'macro_prec_and_rec': [0.22875816993464052, 0.38425925925925924],\n",
       " 'macro_f1': 0.2867861197492472,\n",
       " 'accuracy': 0.34210526315789475}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier # neural network\n",
    "\n",
    "# Classifier\n",
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(3, 3), random_state=1, verbose=True, max_iter=20)\n",
    "# print(clf.get_params())\n",
    "\n",
    "#Fiting trainging data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#predicting the data\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Test')\n",
    "evaluate.evaluate(y_test,y_pred,set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

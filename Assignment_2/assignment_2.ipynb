{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random, itertools\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txt_data():\n",
    "    '''\n",
    "    Master class to load  the dataset\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,path,split=0.9,shuffle=False):\n",
    "        '''\n",
    "        path: path to dataset\n",
    "        split: train/val split\n",
    "        shuffle: shuffle the dataset randomly before splitting or no.\n",
    "        '''\n",
    "        self.path = path\n",
    "        \n",
    "        # list of sentences of list of tuples defining pos_tag and words.\n",
    "        self.data = self.___sent_to_tags___(self.___open___(self)) \n",
    "        if shuffle: random.shuffle(self.data)\n",
    "        self.train =  self.data[:int(len(self.data)*split)]\n",
    "        self.val = self.data[int(len(self.data)*split):]\n",
    "        \n",
    "        self.pos_tags = [i for i in set([j[0] for i in self.data for j in i])] #list of unique pos_tags\n",
    "        self.vocab = [i for i in set([j[1] for i in self.data for j in i])] # list of vocab\n",
    "            \n",
    "    \n",
    "    def ___open___(self,path):\n",
    "        '''\n",
    "        To read a text file\n",
    "        path: path to the text file\n",
    "        '''\n",
    "        with open(self.path, \"r\") as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def ___sent_to_tags___(self,sents):\n",
    "        '''\n",
    "        converts a raw sentence to tags and words as a tuple\n",
    "        '''\n",
    "        dummy = []\n",
    "        for i in tqdm(sents):\n",
    "            d = []\n",
    "            for j in i.split():\n",
    "                try: d.append((j.split('_')[1],j.split('_')[0].lower()))\n",
    "                except: pass #print(f\"--{i}--{j}\\n\")\n",
    "            dummy.append(d)\n",
    "        return dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converitng emission/transition matrix to prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(matrix): \n",
    "    '''\n",
    "    Converitng emission/transition matrix to prob\n",
    "    matrix: list of lists format\n",
    "    \n",
    "    return: prob (2D numpy array)\n",
    "    '''\n",
    "    dummy = []\n",
    "    for i in np.array(matrix):\n",
    "        if np.sum(i) !=0: dummy.append(i/np.sum(i))\n",
    "        else: dummy.append(i)\n",
    "    return np.array(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(matrix, name='simple'):\n",
    "    '''\n",
    "    Smoothing operation to handle zero values\n",
    "    matrix: 2D numpy array\n",
    "    name: name of smoothing technique\n",
    "    \n",
    "    simple smoothing: just adds a fixed number to zeros: 0.000000001*matrix.mean()\n",
    "    simple-good-turing: simply do the average discounting\n",
    "    \n",
    "    return: smoothed prob (numpy array)\n",
    "    '''\n",
    "    matrix = np.array(matrix, dtype= np.float64)\n",
    "    if name == 'simple':\n",
    "        print(f\"Using simple smoothing method\")\n",
    "        matrix[matrix == 0] = 0.000000001*matrix.mean()\n",
    "        return matrix\n",
    "    \n",
    "    elif name == 'simple-gt': \n",
    "        print(f\"Using simple good turing smoothing method\")\n",
    "        for i in range(matrix.shape[0]):\n",
    "            prob = len(matrix[i,:][matrix[i,:] == 1])/len(matrix[i,:])\n",
    "            matrix[i,:][matrix[i,:] != 0] -= 0.7\n",
    "            matrix[i,:][matrix[i,:] == 0] = prob\n",
    "        \n",
    "        matrix[matrix == 0] = 0.0000000001*matrix.mean()\n",
    "        return matrix\n",
    "    \n",
    "    else: raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emission matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_matrix(data,vocab,pos_tags,smooth='simple-gt'):\n",
    "    '''\n",
    "    emission matrix: count of words given the pos_tag\n",
    "    \n",
    "    data: list of sents with the tags: format is list of list of sentences with tag and word as a tuple\n",
    "    vocab: list of vocab\n",
    "    pos_tags: list of pos_tags\n",
    "    smooth: name of smooting technique to use\n",
    "    \n",
    "    return list of pos_tags and a list of matrix (pos_tags,vocab) with count(vocab/pos_tags)\n",
    "    '''\n",
    "    data = [i for j in data for i in j] # list of pos_tag and word pair: [('VB', 'suppose'), ('NP', 'lauren'),()]\n",
    "    dummy = {s[0]:[] for s in data} # dictioanry with tag and list of words as key, value pair, {'AT' :['the', 'the', 'a', 'the'],}\n",
    "    for i in data:\n",
    "        dummy[i[0]].append(i[1])\n",
    "        \n",
    "    e_matrix = [] # emission count matrix with tag and count of words as row, column pair.\n",
    "    for i in tqdm(pos_tags):\n",
    "        if i in dummy.keys():\n",
    "            count = Counter(dummy[i]) # count of words in a list. \n",
    "            e_matrix.append([count[j] for j in vocab]) # vocab(50k) loop takes time not the man loop\n",
    "        else: e_matrix.append([0 for j in vocab])\n",
    "    \n",
    "    return pos_tags, prob(smoothing(e_matrix,name=smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def t_matrix(data, pos_tags, n_gram=2, smooth='simple-gt'):\n",
    "    '''\n",
    "    transition matrix: count of current_pos-tag given the last_pos-tag\n",
    "    \n",
    "    data = list of sents with the tags: format is list of list of sentences with tag and word as a tuple\n",
    "    pos_tags = list of pos_tags\n",
    "    n_gram = look back + 1 i.e, number of words tollok back to calculate the transition matrix\n",
    "    smooth: name of smooting technique to use\n",
    "    \n",
    "    return a list of matrix (pos_tags**n_gram,pos_tags) with count(pos_tags/pos_tags**_gram)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data = [[j[0]  for j in i] for i in data] # list of tags for each sentence: [['AT', 'NN', CS', '.'],[],] \n",
    "    #list of n_grams of tags, for 2 grams [['AT', 'NN'], ['NN', 'MD'], ['MD', 'HV'],[],]\n",
    "    tags = [i[j:j+n_gram] for i in data for j in range(len(i) - n_gram + 1)]\n",
    "    print(f\" Number of {n_gram} grams present in the dataset: {len(tags)/1000000} million\")\n",
    "    # list of tags of cartesian product all the possible combination of pos_tag pair\n",
    "    keys = [' '.join([j for j in i]) for i in list(itertools.product(pos_tags, repeat=n_gram-1))]\n",
    "    keys.append(\"<s>\") # start symbol <s>\n",
    "     \n",
    "    tags_ = {i:[] for i in keys} # initialize an empty dictionary, dict with\n",
    "    for i in tags:\n",
    "        a = ''\n",
    "        for j in i[:-1]:\n",
    "            a+= f\"{j} \"\n",
    "        tags_[a.strip()].append(i[-1])\n",
    "    tags_['<s>'] = [j for i in data for j in i] # list of all the tags in the dataset to count the start_prob\n",
    "    \n",
    "    t_matrix = [] # transition count matrix with n_gram tag and count of pos_tags as key, value pair.\n",
    "    for i in tqdm(keys):\n",
    "        count = Counter(tags_[i])\n",
    "        d = [count[j] for j in pos_tags]\n",
    "        t_matrix.append(d)\n",
    "\n",
    "    return keys[:-1], prob(smoothing(t_matrix[:-1],name=smooth)), prob(smoothing([t_matrix[-1]],name=smooth))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(obs): \n",
    "    '''\n",
    "    Viterbi decoding: \"https://en.wikipedia.org/wiki/Viterbi_algorithm\"\n",
    "    \n",
    "    obs: list of words in the sentence\n",
    "    \n",
    "    return: top path\n",
    "    '''\n",
    "    \n",
    "    global pos_tags\n",
    "    global t_tag\n",
    "    global s_prob\n",
    "    global t_prob\n",
    "    global e_prob\n",
    "    global n_gram\n",
    "    global t_2\n",
    "    \n",
    "    \n",
    "    path = { s:[] for s in pos_tags} # list of all the previous path pos_tags took\n",
    "    curr_prob = {s:s_prob[s]*e_prob[s][obs[0]] for s in pos_tags} # first word/time-step prob\n",
    "    \n",
    "    for i in range(1, len(obs)):\n",
    "        prev_prob = curr_prob\n",
    "        curr_prob = {}\n",
    "        for curr_state in pos_tags:\n",
    "            if i > n_gram - 1 and n_gram > 2: # for the n_gram case\n",
    "                max_prob, state = max(((prev_prob[last_state]*t_prob[f\"{path[curr_state][-1:][0]} {last_state}\"][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            elif n_gram > 2: # simple look back at the last word \n",
    "                max_prob, state = max(((prev_prob[last_state]*t_2[last_state][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            else:\n",
    "                max_prob, state = max(((prev_prob[last_state]*t_prob[last_state][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            curr_prob[curr_state] = max_prob\n",
    "            path[curr_state].append(state)\n",
    "\n",
    "    # find the final largest probability\n",
    "    max_prob = -1\n",
    "    max_path = None\n",
    "    for l, p in path.items():\n",
    "        p.append(l)\n",
    "        if curr_prob[l] > max_prob:\n",
    "            max_path = p\n",
    "            max_prob = curr_prob[l]\n",
    "            \n",
    "    return max_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataset\n",
    "\n",
    "path = \"brown.txt\"\n",
    "brown = txt_data(path)\n",
    "train, val = brown.train, brown.val\n",
    "pos_tags, vocab = brown.pos_tags, brown.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits = 3\n",
    "cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "folds = {} # stores the data of train and test for n_split folds\n",
    "for r, index in tqdm(enumerate(cv.split(brown.data)), total = n_splits):\n",
    "    train = [j for k,j in enumerate(brown.data) if k in index[0]]\n",
    "    test = [j for k,j in enumerate(brown.data) if k in index[1]]\n",
    "    folds[f\"fold_{r}\"]= {\n",
    "                            'train': train,\n",
    "                            'test': test,\n",
    "                            'pos_tags': brown.pos_tags,\n",
    "                            'vocab': brown.vocab\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this cell twice (first time you will wrror, ignore it and run it again)\n",
    "global pos_tags\n",
    "global t_tag\n",
    "global s_prob\n",
    "global t_prob\n",
    "global e_prob\n",
    "global n_gram\n",
    "global t_2\n",
    "\n",
    "\n",
    "n_gram = 2 # markov assumption length: if it's 2 we need trigrams, thus value would be 3 (assumption length + 1) \n",
    "exp_results = {} # saving the results in the dict\n",
    "\n",
    "p = Pool(4)\n",
    "\n",
    "\n",
    "print(f\"Number of folds-/-n_gram used: {len(folds)}-/-{n_gram}\\n\\n\")\n",
    "for fold, data in tqdm(folds.items()):\n",
    "    save_dict = {}\n",
    "    print(f\"\\n\\n{'-'*15}: {fold} \\n\\nlength of train and test data : {len(data['train'])}/{len(data['test'])}\")\n",
    "    \n",
    "    save_dict['n_gram'] =  n_gram\n",
    "    \n",
    "    train = data['train']\n",
    "    val = data['test']\n",
    "    pos_tags = data['pos_tags']\n",
    "    vocab = data['vocab']\n",
    "    \n",
    "    print(f\"Creating the emission and transition probability matrix: \")\n",
    "    e_tag, e_prob = e_matrix(train, vocab, pos_tags) # emission prob and pos_tags\n",
    "    print(f\"Finished emission prob matrix\")\n",
    "    # transition prob and tags\n",
    "    if n_gram == 3: # if n_gram is 3 we also need 2_gram prob for the first index\n",
    "        t_tag, t_prob_2, start_probability = t_matrix(train, pos_tags, 2) \n",
    "        t_tag, t_prob, start_probability = t_matrix(train, pos_tags, n_gram) \n",
    "    if n_gram == 2: t_tag, t_prob, start_probability = t_matrix(train, pos_tags, n_gram) \n",
    "    print(f\"Finished tansition prob matrix\")\n",
    "    \n",
    "    print(f\"Creating emission and transition probability dict for O(1) searching\")\n",
    "    # creating emission and transition probability dict for O(1) searching\n",
    "    if n_gram == 3: t_2 = {i: {pos_tags[f]:j for f,j in enumerate(t_prob_2[r])} for r,i in tqdm(enumerate(pos_tags), total=len(pos_tags))}\n",
    "    else: t_2 = None\n",
    "    t_prob = {i: {pos_tags[f]:j for f,j in enumerate(t_prob[r])} for r,i in tqdm(enumerate(t_tag), total=len(t_tag))}\n",
    "    e_prob = {i: {vocab[f]:j for f,j in enumerate(e_prob[r])} for r,i in tqdm(enumerate(e_tag), total=len(e_tag))}\n",
    "    s_prob = {i:start_probability[r] for r,i in enumerate(pos_tags)} \n",
    "    # start_prob = {i:1.0 for r,i in enumerate(t_tag)} # if wanted the same starting prob for all pos_tags\n",
    "    \n",
    "#     break\n",
    "    print(f\"Starting viterbi decoding: \")\n",
    "    \n",
    "    out = []\n",
    "    true = []\n",
    "    obs = []\n",
    "    for i in val[:3]:\n",
    "        true.append(i)\n",
    "        obs.append([j[1] for j in i])\n",
    "    \n",
    "    for params in tqdm(p.imap(decoding,obs), total=len(obs)):\n",
    "            out.append(list(params))    \n",
    "\n",
    "    save_dict['true_val'] = true\n",
    "    save_dict['pred_val'] = out\n",
    "    \n",
    "    print(f\"starting metric calcualtion\")\n",
    "    true = [i[0] for j in true for i in j]\n",
    "    out = [j for i in out for j in i]\n",
    "    assert len(true) == len(out)\n",
    "\n",
    "    \n",
    "    #pos to index and index to pos dict for easy searching\n",
    "    pos_index = {i:e for e,i in enumerate(pos_tags)}\n",
    "    index_pos = {e:i for e,i in enumerate(pos_tags)}\n",
    "    \n",
    "    save_dict['pos_index'] = pos_index\n",
    "    \n",
    "    # adding the true and predictions in a single list\n",
    "    expected = [pos_index[i] for i in  true]\n",
    "    predicted = [pos_index[i] for i in  out]\n",
    "    \n",
    "    labels = sorted(pos_index.values())\n",
    "    \n",
    "    eps = 0\n",
    "\n",
    "    dummy = OrderedDict()\n",
    "    for i in labels:\n",
    "        dummy[str(i)] = {str(j) : eps for j in labels}\n",
    "    for r, i in enumerate(expected):\n",
    "        dummy[str(i)][str(predicted[r])] +=1\n",
    "    \n",
    "    save_dict['conf_labels'] = [k for k in dummy.keys()]\n",
    "    # confusion matrix\n",
    "    conf_m = np.array([[j for j in i.values()] for i in dummy.values()]) # confusion matrix\n",
    "    save_dict['conf_matrix'] =  conf_m.tolist()\n",
    "    \n",
    "    #classwise \n",
    "    pre = np.sum(conf_m,axis=0)\n",
    "    rec = np.sum(conf_m,axis=1)\n",
    "    p = []\n",
    "    for i in range(len(pre)):\n",
    "        if pre[i] !=0 : p.append(conf_m[i,i]/pre[i])\n",
    "        else: p.append(0)\n",
    "\n",
    "    r = []\n",
    "    for i in range(len(pre)):\n",
    "        if rec[i] !=0 : r.append(conf_m[i,i]/rec[i])\n",
    "        else: r.append(0)\n",
    "\n",
    "    save_dict['classwise_prec_and_rec'] = [p, r]\n",
    "          \n",
    "    # macro\n",
    "    ind = [i for i in range(len(conf_m)) if np.sum(conf_m[:,i]) == 0 and np.sum(conf_m[i,:]) == 0]\n",
    "    conf_m = np.delete(conf_m,ind,1)\n",
    "    conf_m = np.delete(conf_m,ind,0)\n",
    "    pre = np.sum(conf_m,axis=0)\n",
    "    rec = np.sum(conf_m,axis=1)\n",
    "    p = []\n",
    "    for i in range(len(pre)):\n",
    "        if pre[i] !=0 : p.append(conf_m[i,i]/pre[i])\n",
    "        else: p.append(0)\n",
    "\n",
    "    r = []\n",
    "    for i in range(len(pre)):\n",
    "        if rec[i] !=0 : r.append(conf_m[i,i]/rec[i])\n",
    "        else: r.append(0)\n",
    "    save_dict['macro_prec_and_rec'] = [p, r]\n",
    "\n",
    "    print(\"macro --\",sum(p)/len(p), sum(r)/len(r))\n",
    "\n",
    "    # macro f1 score\n",
    "    f_1 = 2*(((sum(p)/len(p))*(sum(r)/len(r))))/((sum(p)/len(p))+(sum(r)/len(r)))\n",
    "    print(f\" Macro F1 score: {f_1}\")\n",
    "    save_dict['macro_f1'] = f_1\n",
    "    \n",
    "    # micro precision and recall\n",
    "    precision = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(pre) \n",
    "    recall = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(rec)\n",
    "    print(\"micro--\",precision, recall)\n",
    "    save_dict['micro_precision_and_recall'] = [precision.tolist(), recall.tolist()]\n",
    "    \n",
    "    # accuracy\n",
    "    acc = len([i for i in range(len(expected)) if expected[i]==predicted[i]])/len(expected)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    save_dict['accuracy'] = acc\n",
    "    \n",
    "    # micro f1 score\n",
    "    f_1 = 2*((precision*recall)/(precision+recall))\n",
    "    print(f\"Micro F1 score: {f_1}\")\n",
    "    save_dict['micro_f1'] = f_1\n",
    "    \n",
    "    exp_results[fold] = save_dict\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving results\n",
    "# with open(\"exp_results.json\", \"w\")as f:\n",
    "#     json.dump(exp_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistics of tag set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading saved results\n",
    "with open('markov_1/stat.json', 'r') as f:\n",
    "    exp_results = json.load(f)\n",
    "\n",
    "# exp_results['fold_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['key_0', 'key_1', 'key_2', 'key_3', 'key_4', 'key_5', 'key_6', 'key_7', 'key_8', 'key_9', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'vocab_len', 'pos_tag_len', 'max_len', 'min_len', 'average_len', 'standard_deviation_len', 'num_word_types_more_than_1_pos_tag', 'num_of_tag_vs_word_types', 'word_type_to_num_of_tag', 'tag_2_num_word_types', 'fold_0_scores_conf_m', 'fold_1_scores_conf_m', 'fold_2_scores_conf_m', 'p_a', 'r_a', 'f_a'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results['fold_0_scores_conf_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-560f22c8d31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstat_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstat_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstat_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_tag_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'brown' is not defined"
     ]
    }
   ],
   "source": [
    "stat_tag = {}\n",
    "\n",
    "stat_tag['vocab_len'] = len(brown.vocab)\n",
    "\n",
    "stat_tag['pos_tag_len'] = len(brown.pos_tags)\n",
    "\n",
    "sent = np.array([len(i) for i in brown.data])\n",
    "\n",
    "stat_tag['max_len'], stat_tag['min_len'], stat_tag['average_len'], stat_tag['standard_deviation_len'] = float(max(sent)), float(min(sent)), np.mean(sent), np.std(sent)\n",
    "\n",
    "tag = {s:[] for s in vocab}\n",
    "\n",
    "for i in brown.data:\n",
    "    for j in i:\n",
    "        tag[j[1]].append(j[0])\n",
    "\n",
    "for s in tag.keys():\n",
    "    tag[s] = len(set(tag[s]))\n",
    "\n",
    "a = sorted([i for i in tag.values() if i >1], reverse=True)\n",
    "    \n",
    "stat_tag['num_word_types_more_than_1_pos_tag'] = len([i for i in tag.values() if i >1])\n",
    "stat_tag['num_of_tag_vs_word_types'] = Counter(a)\n",
    "stat_tag['word_type_to_num_of_tag'] = sorted(tag.items(), key = lambda x: x[1], reverse=True)\n",
    "print(f\"Number of word types having more than 1 tag: {len([i for i in tag.values() if i >1])}\")\n",
    "\n",
    "tag = {s:[] for s in pos_tags}\n",
    "\n",
    "for i in brown.data:\n",
    "    for j in i:\n",
    "        tag[j[0]].append(j[1])\n",
    "\n",
    "for s in tag.keys():\n",
    "    tag[s] = len(set(tag[s]))\n",
    "\n",
    "stat_tag['tag_2_num_word_types'] = sorted(tag.items(), key = lambda x: x[-1], reverse=True)\n",
    "\n",
    "p_a, r_a, f_a = 0,0,0\n",
    "\n",
    "for k,value in exp_results.items():\n",
    "    dummy = {}\n",
    "    \n",
    "    conf_m = np.array(exp_results[k]['conf_matrix'])\n",
    "    dummy['conf_m'] = conf_m.tolist()\n",
    "    #classwise \n",
    "    pre = np.sum(conf_m,axis=0)\n",
    "    rec = np.sum(conf_m,axis=1)\n",
    "    p = []\n",
    "    for i in range(len(pre)):\n",
    "        if pre[i] !=0 : p.append(conf_m[i,i]/pre[i])\n",
    "        else: p.append(0)\n",
    "\n",
    "    r = []\n",
    "    for i in range(len(pre)):\n",
    "        if rec[i] !=0 : r.append(conf_m[i,i]/rec[i])\n",
    "        else: r.append(0)\n",
    "\n",
    "    dummy['macro_classwise_prec_and_rec'] = [p, r]\n",
    "    \n",
    "    # macro\n",
    "    ind = [i for i in range(len(conf_m)) if np.sum(conf_m[:,i]) == 0 and np.sum(conf_m[i,:]) == 0]\n",
    "    conf_m = np.delete(conf_m,ind,1)\n",
    "    conf_m = np.delete(conf_m,ind,0)\n",
    "    pre = np.sum(conf_m,axis=0)\n",
    "    rec = np.sum(conf_m,axis=1)\n",
    "    p = []\n",
    "    for i in range(len(pre)):\n",
    "        if pre[i] !=0 : p.append(conf_m[i,i]/pre[i])\n",
    "        else: p.append(0)\n",
    "\n",
    "    r = []\n",
    "    for i in range(len(pre)):\n",
    "        if rec[i] !=0 : r.append(conf_m[i,i]/rec[i])\n",
    "        else: r.append(0)\n",
    "            \n",
    "    dummy['macro_pre_rec'] = [sum(p)/len(p), sum(r)/len(r)]\n",
    "    print(\"macro --\",sum(p)/len(p), sum(r)/len(r))\n",
    "    p_a += sum(p)/len(p)\n",
    "    r_a += sum(r)/len(r)\n",
    "    # macro f1 score\n",
    "    f_1 = 2*(((sum(p)/len(p))*(sum(r)/len(r))))/((sum(p)/len(p))+(sum(r)/len(r)))\n",
    "    f_a += f_1\n",
    "    print(f\" Macro F1 score: {f_1}\")\n",
    "    dummy['macro_f1_score'] = f_1\n",
    "    \n",
    "    stat_tag[f\"{k}_scores_conf_m\"] = dummy\n",
    "\n",
    "stat_tag['p_a'] = p_a/len(exp_results.keys())\n",
    "stat_tag['r_a'] = r_a/len(exp_results.keys())\n",
    "stat_tag['f_a'] = f_a/len(exp_results.keys())\n",
    "    \n",
    "dummy = {f\"key_{i}\":j for i,j in enumerate(stat_tag.keys())}\n",
    "for k,v in stat_tag.items():\n",
    "    dummy[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving results\n",
    "# with open(\"stat.json\", \"w\")as f:\n",
    "#     json.dump(dummy, f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot words with having more than N tags to get a perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = [i for i in tag.values() if i >1]\n",
    "plt.scatter(range(len(a)),a)\n",
    "len(a), len(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 3\n",
    "###### Which word types are most frequently tagged incorrectly by the HMM, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [j for i in exp_results['fold_0']['pred_val'] for j in i ]\n",
    "true = [j[0]for i in exp_results['fold_0']['true_val']  for j in i ]\n",
    "word = [j[1] for i in exp_results['fold_0']['true_val']  for j in i ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ''\n",
    "incorrect = {s: [] for s in set(word)} # word types:[pred, true] for incorrect predictions\n",
    "for i in range(len(true)): \n",
    "    if pred[i] != true[i] : incorrect[word[i]].append([[pred[i]], true[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_pw = {w:len(v) for w,v in incorrect.items() if len(v)>0}    \n",
    "a += f\"Percentage of words incorrectly classified: {len(wt_pw)/len(set(word))*100}\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_incorrectly = sorted(incorrect.items(), key=lambda x: len(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a+= f\"Word types predicted incorrectly atleast 100 times are: \\n\\n\"\n",
    "z = []\n",
    "for i in most_incorrectly:\n",
    "    a+=f\"word type ({i[0].upper()}) predicted incorrectly {len(i[1])} times \\n\"\n",
    "    z.append(i[0])\n",
    "#     print([i[0],len(i[1])] )\n",
    "    if len(i[1]) <= 100:\n",
    "         break\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ = [i for i in stat_tag['word_type_to_num_of_tag'] if i[0] in z] # word types predictred correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for m in range(1,max(wt_pw.values())+1):\n",
    "    c = 0\n",
    "    for k,v in wt_pw.items():\n",
    "        if v>=m:c+=1\n",
    "#     print(f\"number of words predicted incorrect more than {m} times are {c}\")\n",
    "    a+= f\"number of words predicted incorrect more than {m} times are {c}\\n\"\n",
    "    \n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving question 3 stats\n",
    "# with open('q3.txt', \"w\") as f:\n",
    "#     f.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

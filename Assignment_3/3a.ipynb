{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-2ibXdz8Oy7"
   },
   "source": [
    "# Assignment 3 | Sentiment Analyisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzPLO7KI8LD3"
   },
   "source": [
    "### Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12049,
     "status": "ok",
     "timestamp": 1604509688223,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "80S_izjA6boo",
    "outputId": "2cf0e55c-455a-434b-8242-a5eea147414f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hemant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Requirement already satisfied: tweet-preprocessor in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: emosent-py in /opt/anaconda/anaconda3/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied: emoji in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from emosent import get_emoji_sentiment_rank\n",
    "\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#installing tweet-preprocessor\n",
    "from tqdm.auto import tqdm\n",
    "!pip install tweet-preprocessor --user\n",
    "import preprocessor as p\n",
    "from pathlib import Path\n",
    "!pip install emosent-py --user\n",
    "#emoji library\n",
    "!pip install emoji --user\n",
    "import emoji\n",
    "\n",
    "# model save\n",
    "import joblib\n",
    "\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "executionInfo": {
     "elapsed": 10016,
     "status": "ok",
     "timestamp": 1604509688225,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "2vO89PLYLdSu"
   },
   "outputs": [],
   "source": [
    "class featurise(object):\n",
    "    def __init__(self, x,y):\n",
    "        super(featurise, self).__init__()\n",
    "        self.seq = None\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        ##########################################\n",
    "        self.final_df = pd.DataFrame()\n",
    "        self.final_df['tweets'] = x\n",
    "        self.final_df['target'] = y\n",
    "        ##########################################\n",
    "        self.negation_w = set(['never', 'no', 'nothing', 'nowhere', 'noone', 'none', 'not', 'havent', 'hasnt', 'hadnt', 'cant', \n",
    "                         'couldnt', 'shouldnt', 'wont', 'wouldnt', 'dont', 'doesnt', 'didnt', 'isnt', 'arent', 'aint'] )\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.stop_words.remove('not')\n",
    "        \n",
    "        self.nrc = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/7. NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\", header=None, sep=\"\\t\")\n",
    "        self.dic = {self.nrc[0][i]:self.nrc[1][i] for i in range(len(self.nrc))}\n",
    "        self.mpqa_df = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/mpqa.txt\",header=None, sep=\"\\t\")\n",
    "        self.dic_mpqa = self.mpqa_df.set_index(0).to_dict()\n",
    "        self.bingliu_df = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/1. BingLiu.csv\", header=None, sep=\"\\t\")\n",
    "        self.dic_bing = self.bingliu_df.set_index(0).to_dict()\n",
    "        self.sent140_df = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/3. Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\", header=None, sep=\"\\t\")\n",
    "        self.dic_sent = self.sent140_df.set_index(0).to_dict()\n",
    "\n",
    "    def elongated_word(self, x):\n",
    "        count = 0\n",
    "        regex_ = re.compile(r\"(.)\\1{2}\")\n",
    "        for i in x.split():            \n",
    "            if regex_.search(i): count+=1\n",
    "        return count\n",
    "        \n",
    "    def emoji(self, x):\n",
    "        emo_final = []\n",
    "        for word_ in tqdm(x):\n",
    "            emoji_ = {d:word for d,word in enumerate(regex.findall(r'\\X', word_)) if any(char in emoji.UNICODE_EMOJI for char in word)}\n",
    "\n",
    "            dummy = [0,0,0]\n",
    "            for i,e in emoji_.items():\n",
    "                if i == len(word_) - 1:\n",
    "                    dummy[-1] += 1\n",
    "                    continue\n",
    "                elif get_emoji_sentiment_rank(e)['positive'] > get_emoji_sentiment_rank(e)['negative']: dummy[0] += 1\n",
    "                elif get_emoji_sentiment_rank(e)['positive'] < get_emoji_sentiment_rank(e)['negative']: dummy[1] += 1\n",
    "            emo_final.append(dummy)\n",
    "        emo_final = np.array(emo_final)\n",
    "        self.final_df['p+'] = emo_final[:,0]\n",
    "        self.final_df['p-'] = emo_final[:,1]\n",
    "        self.final_df['p+-'] = emo_final[:,2]\n",
    "    \n",
    "    def punctuation(self, x):\n",
    "        regex = re.compile(r\"([?|!])\\1{0}\")\n",
    "        dummy = []\n",
    "        for i in tqdm(x):\n",
    "            d = [len([word for word in i.split() if regex.search(word)])]\n",
    "            if regex.search(i.split()[-1]): d.append(1)\n",
    "            else: d.append(0)\n",
    "            dummy.append(d)\n",
    "        dummy = np.array(dummy)\n",
    "        self.final_df['punctu_1'] = dummy[:,0]\n",
    "        self.final_df['punctu_2'] = dummy[:,1]\n",
    "                   \n",
    "    def all_caps(self, x):\n",
    "        return len([1 for each in x.split() if each.isupper()])\n",
    "\n",
    "    def hashtag_count(self, x):\n",
    "        return len([1 for word in x.split() if word.startswith('#') and len(word)>1 ])\n",
    "    \n",
    "    def negation(self, x):\n",
    "        flag = 0\n",
    "        splits = \",.:;!?\"\n",
    "        final = []\n",
    "        count = 0\n",
    "#         x = f\"I don't think I will enjoy it: it might be too spicy.\".lower()\n",
    "        for word in x.split():\n",
    "            if flag ==1:\n",
    "                for symb in splits:\n",
    "                    if symb in word:\n",
    "                        flag = 0\n",
    "                        word_split = word.split(symb)\n",
    "                        \n",
    "                        if len(word_split[0])>0:\n",
    "                        \n",
    "                            count+=1\n",
    "                        \n",
    "                            if len(word_split[-1])>1:\n",
    "                        \n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                if flag==1:\n",
    "                    \n",
    "                    count+=1\n",
    "                \n",
    "            elif flag==0:\n",
    "                \n",
    "                pass\n",
    "\n",
    "            clean = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "            if clean in self.negation_w:\n",
    "                flag=1\n",
    "        return count\n",
    "\n",
    "    def pos_tag_feat(self, a):\n",
    "        print('pos_tags')\n",
    "        tagged_texts = [pos_tag_sents([i.split()])[0] for i in tqdm(a)]\n",
    "\n",
    "        dummy = [Counter([j[-1] for j in i]) for i in tqdm(tagged_texts)]\n",
    "        pos_vocab = set([j for i in dummy for j in i.keys()])\n",
    "        pos_t_i = {i:d for d,i in enumerate(pos_vocab)}\n",
    "        a = np.zeros((len(tagged_texts),len(pos_vocab)))\n",
    "\n",
    "        for d,i in tqdm(enumerate(dummy)):\n",
    "            for k,v in i.items():\n",
    "                a[d][pos_t_i[k]] = v\n",
    "        for d,i in enumerate(pos_t_i.keys()):\n",
    "            self.final_df[i] = a[:,d]\n",
    "#         return pd.DataFrame(a,columns=pos_t_i.keys())\n",
    "\n",
    "    #########################\n",
    "    \n",
    "    def remove_stopword(self, x):\n",
    "        dummy = []\n",
    "        for i in x.split():\n",
    "            if i not in self.stop_words: dummy.append(i)\n",
    "        return ' '.join(dummy)\n",
    "    \n",
    "    def all_lower(self, x):\n",
    "        return x.lower()\n",
    "    def remove_punctuation(self, x):\n",
    "        \n",
    "        return re.sub(r'[^\\w\\s]', '', x)\n",
    "    def remove_number(self, x):\n",
    "        return ''.join([_ for _ in x if not _.isdigit()])\n",
    "    def rem_un_url(self, x):\n",
    "        # return p.clean(x)\n",
    "        result = re.sub(r\"http\\S+\", \"\", x)\n",
    "        return re.sub('@[^\\s]+','',result)\n",
    "\n",
    "    #################################################################3\n",
    "    def nrc_emotion_score(self,x):\n",
    "\n",
    "        score = 0\n",
    "        for each in  x.lower().split():\n",
    "            if each in self.dic: score += self.dic[each]\n",
    "        return score\n",
    "    \n",
    "    def polar_word_count_mpqa(self, x):\n",
    "\n",
    "        pos,neg = 0,0\n",
    "        for each in  x.split():\n",
    "            if each.lower() in self.dic_mpqa[1]:\n",
    "                if self.dic_mpqa[1][each.lower()] == 'negative': neg+=1\n",
    "                else: pos+=1\n",
    "        return pos,neg\n",
    "    \n",
    "    def polar_word_count_bingliu(self, x):\n",
    "        pos,neg = 0,0\n",
    "        for each in  x.split():\n",
    "            if each.lower() in self.dic_bing[1]:\n",
    "                if self.dic_bing[1][each.lower()] == 'negative': neg+=1\n",
    "                else: pos+=1\n",
    "        return pos,neg\n",
    "\n",
    "    \n",
    "    def agg_polarity_score_sent140(self,x):\n",
    "        \n",
    "        pos,neg = 0,0\n",
    "        for each in  x.lower().split():\n",
    "            if each in self.dic_sent[2]:\n",
    "                pos+=self.dic_sent[2][each]\n",
    "            if each in self.dic_sent[3]:\n",
    "                neg+=self.dic_sent[3][each]\n",
    "        return pos, neg\n",
    "    \n",
    "    def process(self):\n",
    "        self.final_df['elongated'] = tqdm(self.x.map(self.elongated_word))\n",
    "        self.emoji(list(self.x))\n",
    "        self.punctuation(list(self.x))\n",
    "\n",
    "        self.final_df['allcaps'] = [self.all_caps(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['hashtag'] = [self.hashtag_count(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['negation'] = [self.negation(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['negation'] = [self.negation(i)  for i in tqdm(self.x)]\n",
    "        \n",
    "        print('shit')\n",
    "        self.final_df['nrc_emotion_score'] = [self.nrc_emotion_score(i) for i in tqdm(self.x)]\n",
    "        dummy = np.array([self.polar_word_count_mpqa(i) for i in tqdm(self.x)])\n",
    "        self.final_df[\"pos_mpqa\"] = dummy[:,0]\n",
    "        self.final_df[\"neg_mpqa\"] = dummy[:,1]\n",
    "        dummy = np.array([self.polar_word_count_bingliu(i) for i in tqdm(self.x)])\n",
    "        self.final_df['pos_bilingu'] = dummy[:,0]\n",
    "        self.final_df['neg_bilingu'] = dummy[:,1]\n",
    "        dummy = np.array([self.agg_polarity_score_sent140(i) for i in tqdm(self.x)])\n",
    "        self.final_df['pos_sent140'] = dummy[:,0]\n",
    "        self.final_df['neg_sent140'] = dummy[:,1]\n",
    "\n",
    "        \n",
    "        self.final_df['new_tweets'] = [self.rem_un_url(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['new_tweets'] = [self.all_lower(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        self.final_df['new_tweets'] = [self.remove_number(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        \n",
    "        self.pos_tag_feat(list(self.final_df['new_tweets']))\n",
    "        \n",
    "        self.final_df['new_tweets'] = [self.remove_punctuation(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        print(\"shit\")\n",
    "        self.final_df['new_tweets'] = [self.remove_stopword(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        return self\n",
    "\n",
    "################################################################################\n",
    "def printdf(df, rows = 5):\n",
    "    print(df[:rows].to_markdown())\n",
    "    return True \n",
    "  \n",
    "#unigrams bow\n",
    "def bow(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "\n",
    "    count_vect = CountVectorizer() #in scikit-learn\n",
    "    count_vect.fit(X_train)\n",
    "\n",
    "    bow_counts_train = count_vect.transform(X_train)\n",
    "    bow_counts_cv = count_vect.transform(X_cv)\n",
    "    bow_counts_test = count_vect.transform(X_test)\n",
    "    print(\"the shape of out text BOW vectorizer \",bow_counts_train.get_shape())\n",
    "    print(\"the number of unique words \", bow_counts_train.get_shape()[1])\n",
    "    # final_counts_test=final_counts_test.toarray()\n",
    "    return count_vect, bow_counts_train, bow_counts_cv, bow_counts_test\n",
    "\n",
    "#bigrams bow\n",
    "def bigram(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "    count_vect = CountVectorizer(ngram_range=(2,2))\n",
    "    bigram_counts_train = count_vect.fit_transform(X_train)\n",
    "    bigram_counts_cv = count_vect.transform(X_cv)\n",
    "    bigram_counts_test = count_vect.transform(X_test)\n",
    "    print(\"some feature names \", count_vect.get_feature_names()[:10])\n",
    "    print(\"the shape of out  BIGRAM vectorizer \",bigram_counts_train.get_shape())\n",
    "    print(\"the number of unique words with bigrams \", bigram_counts_train.get_shape()[1])\n",
    "    return count_vect, bigram_counts_train, bigram_counts_cv, bigram_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6820,
     "status": "ok",
     "timestamp": 1604509829782,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "FtGOG6pM7nKC",
    "outputId": "28df25b8-3d3d-4409-b6e4-745416caf55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120000, 6)\n",
      "(480000, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/3a/sentiment_train.csv\")\n",
    "test = pd.read_csv(\"data/3a/sentiment_test.csv\")\n",
    "\n",
    "# uncomment this later \n",
    "# train = train[:30000]\n",
    "# test = test[:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"5\"]\n",
    "y_train = train[\"0\"]\n",
    "X_test = test[\"5\"]\n",
    "y_test = test['0']\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27228,
     "status": "ok",
     "timestamp": 1604509710650,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "Qwf3CBZ223dJ",
    "outputId": "2479cf50-9ab4-4b3c-8cfc-e14b087555ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Found\n",
      "CSV Found\n",
      "CSV Found\n"
     ]
    }
   ],
   "source": [
    "#### FEATURISATION\n",
    "\n",
    "my_file = Path(\"data/3a/train_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    train_df = pd.read_csv(\"data/3a/train_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise train data\n",
    "    proc = featurise(X_train,y_train).process()\n",
    "    train_df = proc.final_df\n",
    "    train_df.to_csv('data/3a/train_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/val_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    val_df = pd.read_csv(\"data/3a/val_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise cv data\n",
    "    proc = featurise(X_cv,y_cv).process()\n",
    "    val_df = proc.final_df\n",
    "    val_df.to_csv('data/3a/val_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/test_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    test_df = pd.read_csv(\"data/3a/test_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise test data\n",
    "    proc = featurise(X_test,y_test).process()\n",
    "    test_df = proc.final_df\n",
    "    test_df.to_csv('data/3a/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweets</th>\n",
       "      <th>target</th>\n",
       "      <th>elongated</th>\n",
       "      <th>p+</th>\n",
       "      <th>p-</th>\n",
       "      <th>p+-</th>\n",
       "      <th>punctu_1</th>\n",
       "      <th>punctu_2</th>\n",
       "      <th>allcaps</th>\n",
       "      <th>...</th>\n",
       "      <th>VBD</th>\n",
       "      <th>POS</th>\n",
       "      <th>EX</th>\n",
       "      <th>CD</th>\n",
       "      <th>#</th>\n",
       "      <th>DT</th>\n",
       "      <th>CC</th>\n",
       "      <th>,</th>\n",
       "      <th>NNPS</th>\n",
       "      <th>VBZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>990207</td>\n",
       "      <td>okay. i promise that by the end of this month,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1036353</td>\n",
       "      <td>sorry to much tweeting</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1006352</td>\n",
       "      <td>i hope it doesn't rain tomorrow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>634832</td>\n",
       "      <td>@puerhan I modified some things, but the core ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>413665</td>\n",
       "      <td>@tracyewilli LOL! I'll bet</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             tweets  target  \\\n",
       "0      990207  okay. i promise that by the end of this month,...       0   \n",
       "1     1036353                            sorry to much tweeting        4   \n",
       "2     1006352                   i hope it doesn't rain tomorrow        0   \n",
       "3      634832  @puerhan I modified some things, but the core ...       4   \n",
       "4      413665                        @tracyewilli LOL! I'll bet        4   \n",
       "\n",
       "   elongated  p+  p-  p+-  punctu_1  punctu_2  allcaps  ...  VBD  POS   EX  \\\n",
       "0          0   0   0    0         0         0        0  ...  0.0  0.0  0.0   \n",
       "1          0   0   0    0         0         0        0  ...  0.0  0.0  0.0   \n",
       "2          0   0   0    0         0         0        0  ...  0.0  0.0  0.0   \n",
       "3          0   0   0    0         1         1        1  ...  1.0  0.0  0.0   \n",
       "4          0   0   0    0         1         0        1  ...  0.0  0.0  0.0   \n",
       "\n",
       "    CD    #   DT   CC    ,  NNPS  VBZ  \n",
       "0  0.0  0.0  3.0  0.0  0.0   0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "3  0.0  0.0  3.0  1.0  0.0   0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "executionInfo": {
     "elapsed": 537824,
     "status": "ok",
     "timestamp": 1604498381196,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "sFYM9N42oFnK"
   },
   "outputs": [],
   "source": [
    "# train_df.isnull().values.any()\n",
    "# train_df['new_tweets'].isnull().sum()\n",
    "# df['your column name'].isnull().sum()\n",
    "train_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "val_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "test_df.dropna(subset = [\"new_tweets\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593820,
     "status": "ok",
     "timestamp": 1604498441480,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "d8kHJBcC34W_",
    "outputId": "298c059a-e285-4272-9e4e-36d1ee8c0744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of out text BOW vectorizer  (747244, 253751)\n",
      "the number of unique words  253751\n"
     ]
    }
   ],
   "source": [
    "count_vect, bow_counts_train, bow_counts_cv, bow_counts_test = bow(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])\n",
    "# count_vect_, bigram_counts_train, bigram_counts_cv, bigram_counts_test = bigram(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<368070x253751 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2496945 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_counts_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253751,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-969646677122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_counts_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \"\"\"\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bow_counts_cv.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = pd.DataFrame(bow_counts_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.sparse.to_dense().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "executionInfo": {
     "elapsed": 610735,
     "status": "ok",
     "timestamp": 1604498462093,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "1u2OIERBS07X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from scipy import sparse\n",
    "# def save_npz(f,name):\n",
    "#     sparse.save_npz(\"data/3a/npz/\"+str(name)+\".npz\", f)\n",
    "#     return True\n",
    "\n",
    "# save_npz(bow_counts_cv, \"bow_counts_cv\")\n",
    "# save_npz(bow_counts_test, \"bow_counts_test\")\n",
    "# save_npz(bow_counts_train, \"bow_counts_train\")\n",
    "# save_npz(bigram_counts_train, \"bigram_counts_train\")\n",
    "# save_npz(bigram_counts_test, \"bigram_counts_test\")\n",
    "# save_npz(bigram_counts_cv, \"bigram_counts_cv\")\n",
    "\n",
    "# # train_df.to_csv('data/3a/gram_traincsv.csv')\n",
    "# # val_df.to_csv('data/3a/gram_valcsv.csv')\n",
    "# # test_df.to_csv('data/3a/gram_testcsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1604491907199,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "XEkHXz3XCCMb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1604493558672,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "rJvJUSIwJDmX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2FMJ3VHPU0ttKEQbYUYon",
   "collapsed_sections": [],
   "mount_file_id": "1rSUMnud92JcJuJyZ5Rh2ITMs9pKt7nE3",
   "name": "3a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

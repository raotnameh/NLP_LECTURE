{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assignment_2.ipynb  brown.txt  question.pdf\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random, itertools\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txt_data():\n",
    "    '''\n",
    "    Master class to load dataset\n",
    "    '''\n",
    "    def __init__(self,path,split=0.9):\n",
    "        self.path = path\n",
    "        \n",
    "        # list of sentences of list of tuples defining pos_tag and words.\n",
    "        self.data = self.___sent_to_tags___(self.___open___(self)) \n",
    "#         random.shuffle(self.data)\n",
    "        self.train =  self.data[:int(len(self.data)*split)]\n",
    "        self.val = self.data[int(len(self.data)*split):]\n",
    "        \n",
    "        self.pos_tags = [i for i in set([j[0] for i in self.data for j in i])] #list of unique pos_tags\n",
    "        self.vocab = [i for i in set([j[1] for i in self.data for j in i])] # list of vocab\n",
    "            \n",
    "    \n",
    "    def ___open___(self,path):\n",
    "        '''\n",
    "        To read a text file\n",
    "        path: path to the text file\n",
    "        '''\n",
    "        with open(self.path, \"r\") as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def ___sent_to_tags___(self,sents):\n",
    "        '''\n",
    "        '''\n",
    "        dummy = []\n",
    "        for i in tqdm(sents):\n",
    "            d = []\n",
    "            for j in i.split():\n",
    "                try: d.append((j.split('_')[1],j.split('_')[0].lower()))\n",
    "                except: pass #print(f\"--{i}--{j}\\n\")\n",
    "            dummy.append(d)\n",
    "        return dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converitng emission/transition matrix to prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(matrix): \n",
    "    dummy = []\n",
    "    for i in np.array(matrix):\n",
    "        if np.sum(i) !=0: dummy.append(i/np.sum(i))\n",
    "        else: dummy.append(i)\n",
    "    return np.array(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooting(matrix, name='simple'):\n",
    "    if name == 'simple':\n",
    "        matrix[matrix == 0] = 0.000000001*matrix.mean()\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emission matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_matrix(data,vocab,pos_tags):\n",
    "    '''\n",
    "    emission matrix: count of words given the pos_tag\n",
    "    \n",
    "    data = list of sents with the tags: format is list of list of sentences with tag and word as a tuple\n",
    "    vocab = list of vocab\n",
    "    pos_tags = list of pos_tags\n",
    "    \n",
    "    return list of pos_tags and a list of matrix (pos_tags,vocab) with count(vocab/pos_tags)\n",
    "    '''\n",
    "    data = [i for j in data for i in j] # list of pos_tag and word pair: [('VB', 'suppose'), ('NP', 'lauren'),()]\n",
    "    dummy = {s[0]:[] for s in data} # dictioanry with tag and list of words as key, value pair, {'AT' :['the', 'the', 'a', 'the'],}\n",
    "    for i in data:\n",
    "        dummy[i[0]].append(i[1])\n",
    "        \n",
    "    e_matrix = [] # emission count matrix with tag and count of words as row, column pair.\n",
    "    for i in tqdm(pos_tags):\n",
    "        if i in dummy.keys():\n",
    "            count = Counter(dummy[i]) # count of words in a list. \n",
    "            e_matrix.append([count[j] for j in vocab]) # vocab(50k) loop takes time not the man loop\n",
    "        else: e_matrix.append([0 for j in vocab])\n",
    "    \n",
    "    return pos_tags, prob(e_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def t_matrix(data, pos_tags, n_gram=2):\n",
    "    '''\n",
    "    transition matrix: count of current_pos-tag given the last_pos-tag\n",
    "    \n",
    "    data = list of sents with the tags: format is list of list of sentences with tag and word as a tuple\n",
    "    pos_tags = list of pos_tags\n",
    "    n_gram = look back + 1 i.e, number of words tollok back to calculate the transition matrix\n",
    "    \n",
    "    return a list of matrix (pos_tags**n_gram,pos_tags) with count(pos_tags/pos_tags**_gram)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data = [[j[0]  for j in i] for i in data] # list of tags for each sentence: [['AT', 'NN', CS', '.'],[],] \n",
    "    #list of n_grams of tags, for 2 grams [['AT', 'NN'], ['NN', 'MD'], ['MD', 'HV'],[],]\n",
    "    tags = [i[j:j+n_gram] for i in data for j in range(len(i) - n_gram + 1)]\n",
    "    print(f\" Number of {n_gram} grams present in the dataset: {len(tags)/1000000} million\")\n",
    "    # list of tags of cartesian product all the possible combination of pos_tag pair\n",
    "    keys = [' '.join([j for j in i]) for i in list(itertools.product(pos_tags, repeat=n_gram-1))]\n",
    "    keys.append(\"<s>\") # start symbol <s>\n",
    "     \n",
    "    tags_ = {i:[] for i in keys} # initialize an empty dictionary, dict with\n",
    "    for i in tags:\n",
    "        a = ''\n",
    "        for j in i[:-1]:\n",
    "            a+= f\"{j} \"\n",
    "        tags_[a.strip()].append(i[-1])\n",
    "    tags_['<s>'] = [j for i in data for j in i] # list of all the tags in the dataset to count the start_prob\n",
    "    \n",
    "    t_matrix = [] # transition count matrix with n_gram tag and count of pos_tags as key, value pair.\n",
    "    for i in tqdm(keys):\n",
    "        count = Counter(tags_[i])\n",
    "        d = [count[j] for j in pos_tags]\n",
    "        t_matrix.append(d)\n",
    "\n",
    "    return keys[:-1], prob(t_matrix[:-1]), prob([t_matrix[-1]])[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(obs, pos_tags, t_tag, s_prob, t_prob, e_prob, n_gram, t_2=None):\n",
    "    '''\n",
    "    Viterbi decoding: \"https://en.wikipedia.org/wiki/Viterbi_algorithm\"\n",
    "    \n",
    "    obs: list of words in the sentence\n",
    "    pos_tags: list of pos_tags\n",
    "    s_prob: starting probabilty\n",
    "    t_prob: transition probability\n",
    "    e_prob: emission probability\n",
    "    n_gram: number of words to look back +1\n",
    "    \n",
    "    return: top path\n",
    "    '''\n",
    "    path = { s:[] for s in pos_tags} # list of all the previous path pos_tags took\n",
    "    curr_prob = {s:s_prob[s]*e_prob[s][obs[0]] for s in pos_tags} # first word/time-step prob\n",
    "\n",
    "    for i in range(1, len(obs)):\n",
    "        prev_prob = curr_prob\n",
    "        curr_prob = {}\n",
    "        for curr_state in pos_tags:\n",
    "            if i > n_gram - 1 and n_gram > 2: # for the n_gram case\n",
    "                max_prob, state = max(((prev_prob[last_state]*t_prob[f\"{path[curr_state][-1:][0]} {last_state}\"][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            elif n_gram > 2: # simple look back at the last word \n",
    "                max_prob, state = max(((prev_prob[last_state]*t_2[last_state][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            else:\n",
    "                max_prob, state = max(((prev_prob[last_state]*t_prob[last_state][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            curr_prob[curr_state] = max_prob\n",
    "            path[curr_state].append(state)\n",
    "\n",
    "    # find the final largest probability\n",
    "    max_prob = -1\n",
    "    max_path = None\n",
    "    for l, p in path.items():\n",
    "        p.append(l)\n",
    "        if curr_prob[l] > max_prob:\n",
    "            max_path = p\n",
    "            max_prob = curr_prob[l]\n",
    "            \n",
    "    return max_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4553111a9bc4222a5280ccde1fa8486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=55145.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"brown.txt\"\n",
    "brown = txt_data(path)\n",
    "train, val = brown.train, brown.val\n",
    "# pos_tags, vocab = brown.pos_tags, brown.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7491c86db24994a3cc0bcd1a882603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "folds = {} # stores the data of train and test for n_split folds\n",
    "for r, index in tqdm(enumerate(cv.split(brown.data)), total = n_splits):\n",
    "    train = [j for k,j in enumerate(brown.data) if k in index[0]]\n",
    "    test = [j for k,j in enumerate(brown.data) if k in index[1]]\n",
    "    folds[f\"fold_{r}\"]= {\n",
    "                            'train': train,\n",
    "                            'test': test,\n",
    "                            'pos_tags': brown.pos_tags,\n",
    "                            'vocab': brown.vocab\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds-/-n_gram used: 3-/-2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecae67ff8f94562b1cfdedf9b2862ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------: fold_0 \n",
      "\n",
      "length of train and test data : 36763/18382\n",
      "Creating the emission and transition probability matrix: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b3fa4fb19448d3a05e17dbc9d4ac06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished emission prob matrix\n",
      " Number of 2 grams present in the dataset: 0.738769 million\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2237f6c67ea249069b63194e4dd106b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished tansition prob matrix\n",
      "Creating emission and transition probability dict for O(1) searching\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9ba8c1a386405290466754736723c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4310888be74d4bf29f93da50ac78d8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting viterbi decoding: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cc6b0f535a470ba07a5a18e539e7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting metric calcualtion\n",
      "micro-- 0.813953488372093 0.813953488372093\n",
      "Accuracy: 0.813953488372093\n",
      "F1 score: 0.8139534883720931\n",
      "\n",
      "\n",
      "---------------: fold_1 \n",
      "\n",
      "length of train and test data : 36763/18382\n",
      "Creating the emission and transition probability matrix: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-3f3718ea581f>:98: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision = [i[r] for r,i in enumerate(conf_m)]/pre\n",
      "<ipython-input-42-3f3718ea581f>:99: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = [i[r] for r,i in enumerate(conf_m)]/rec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d5671e7914834b8ed539c2dc46782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished emission prob matrix\n",
      " Number of 2 grams present in the dataset: 0.737799 million\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9402a8b22d2b4a948a37cff4ae8ad2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished tansition prob matrix\n",
      "Creating emission and transition probability dict for O(1) searching\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ecc59e791648c7b98aef727d4f6810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd578ab0bfc48dea263ebacdf2ea656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting viterbi decoding: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dec63e049f6478581519e0ebbc8d4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting metric calcualtion\n",
      "micro-- 0.84 0.84\n",
      "Accuracy: 0.84\n",
      "F1 score: 0.8399999999999999\n",
      "\n",
      "\n",
      "---------------: fold_2 \n",
      "\n",
      "length of train and test data : 36764/18381\n",
      "Creating the emission and transition probability matrix: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69490a4820aa48b98bcc63a9df7b9ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished emission prob matrix\n",
      " Number of 2 grams present in the dataset: 0.735526 million\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb76dafdb716455caccb310761a6d6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished tansition prob matrix\n",
      "Creating emission and transition probability dict for O(1) searching\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a0d25b59bc4fab91df7b8a5424ab74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c2114743624597ae9959bc3f1a5433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting viterbi decoding: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b68ebf1ea340ec8e38a2f0af205e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting metric calcualtion\n",
      "micro-- 0.8285714285714286 0.8285714285714286\n",
      "Accuracy: 0.8285714285714286\n",
      "F1 score: 0.8285714285714286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gram = 2 # markov assumption length: if it's 2 we need trigrams, thus value would be 3 (assumption length + 1) \n",
    "exp_results = {} # saving the results in the dict\n",
    "\n",
    "print(f\"Number of folds-/-n_gram used: {len(folds)}-/-{n_gram}\\n\\n\")\n",
    "for fold, data in tqdm(folds.items()):\n",
    "    save_dict = {}\n",
    "    print(f\"\\n\\n{'-'*15}: {fold} \\n\\nlength of train and test data : {len(data['train'])}/{len(data['test'])}\")\n",
    "    \n",
    "    save_dict['n_gram'] =  n_gram\n",
    "    \n",
    "    train = data['train']\n",
    "    val = data['test']\n",
    "    pos_tags = data['pos_tags']\n",
    "    vocab = data['vocab']\n",
    "    \n",
    "#     save_dict['train_data'] =  train\n",
    "#     save_dict['val_data'] = val\n",
    "    save_dict['pos_tags'] = pos_tags\n",
    "    save_dict['vocab'] = vocab\n",
    "\n",
    "    print(f\"Creating the emission and transition probability matrix: \")\n",
    "    e_tag, e_prob = e_matrix(train, vocab, pos_tags) # emission prob and pos_tags\n",
    "    print(f\"Finished emission prob matrix\")\n",
    "    # transition prob and tags\n",
    "    if n_gram == 3: # if n_gram is 3 we also need 2_gram prob for the first index\n",
    "        t_tag, t_prob_2, start_probability = t_matrix(train, pos_tags, 2) \n",
    "        t_tag, t_prob, start_probability = t_matrix(train, pos_tags, n_gram) \n",
    "    if n_gram == 2: t_tag, t_prob, start_probability = t_matrix(train, pos_tags, n_gram) \n",
    "    print(f\"Finished tansition prob matrix\")\n",
    "    # smoothing the probabilites\n",
    "    e_prob[e_prob == 0] = 0.000000001*e_prob.mean()\n",
    "    t_prob[t_prob == 0] = 0.000000001*t_prob.mean()\n",
    "    \n",
    "    if n_gram == 3: \n",
    "        save_dict['t_prob_t_prob_2']  = (t_prob,t_prob_2)\n",
    "        save_dict['e_prob'] = e_prob\n",
    "    elif n_gram == 2:\n",
    "        save_dict['t_prob']  = (t_prob)\n",
    "        save_dict['e_prob'] = e_prob\n",
    "    \n",
    "    print(f\"Creating emission and transition probability dict for O(1) searching\")\n",
    "    # creating emission and transition probability dict for O(1) searching\n",
    "    if n_gram == 3: t_2 = {i: {pos_tags[f]:j for f,j in enumerate(t_prob_2[r])} for r,i in tqdm(enumerate(pos_tags), total=len(pos_tags))}\n",
    "    else: t_2 = None\n",
    "    transition_probability = {i: {pos_tags[f]:j for f,j in enumerate(t_prob[r])} for r,i in tqdm(enumerate(t_tag), total=len(t_tag))}\n",
    "    emission_probability = {i: {vocab[f]:j for f,j in enumerate(e_prob[r])} for r,i in tqdm(enumerate(e_tag), total=len(e_tag))}\n",
    "    start_prob = {i:start_probability[r] for r,i in enumerate(pos_tags)} \n",
    "    # start_prob = {i:1.0 for r,i in enumerate(t_tag)} # if wanted the same starting prob for all pos_tags\n",
    "    \n",
    "    print(f\"Starting viterbi decoding: \")\n",
    "    out = []\n",
    "    true = []\n",
    "    for i in tqdm(val[:1]):\n",
    "        true.append(i)\n",
    "        obs = [j[1] for j in i]\n",
    "        out.append(decoding(obs, pos_tags, t_tag, start_prob, transition_probability, emission_probability, n_gram, t_2))\n",
    "    \n",
    "    save_dict['true_val'] = true\n",
    "    save_dict['pred_val'] = out\n",
    "    \n",
    "    print(f\"starting metric calcualtion\")\n",
    "    true = [i[0] for j in true for i in j]\n",
    "    out = [j for i in out for j in i]\n",
    "    assert len(true) == len(out)\n",
    "\n",
    "    \n",
    "    #pos to index and index to pos dict for easy searching\n",
    "    pos_index = {i:e for e,i in enumerate(pos_tags)}\n",
    "    index_pos = {e:i for e,i in enumerate(pos_tags)}\n",
    "    \n",
    "    save_dict['pos_index'] = pos_index\n",
    "    save_dict['index_pos'] = index_pos\n",
    "    \n",
    "    # adding the true and predictions in a single list\n",
    "    expected = [pos_index[i] for i in  true]\n",
    "    predicted = [pos_index[i] for i in  out]\n",
    "    \n",
    "#     labels = sorted(pos_index.values())\n",
    "    labels = set(expected+predicted)\n",
    "    save_dict['labels'] = labels\n",
    "    \n",
    "    \n",
    "    eps = 0 #0.1e-15\n",
    "\n",
    "    dummy = OrderedDict()\n",
    "    for i in labels:\n",
    "        dummy[str(i)] = {str(j) : eps for j in labels}\n",
    "    for r, i in enumerate(expected):\n",
    "        dummy[str(i)][str(predicted[r])] +=1\n",
    "\n",
    "    # confusion matrix\n",
    "    conf_m = np.array([[j for j in i.values()] for i in dummy.values()]) # confusion matrix\n",
    "    save_dict['conf_matrix'] =  conf_m\n",
    "    \n",
    "    #classwise \n",
    "#     pre = np.sum(conf_m,axis=0)\n",
    "#     rec = np.sum(conf_m,axis=1)\n",
    "#     precision = [i[r] for r,i in enumerate(conf_m)]/pre \n",
    "#     recall = [i[r] for r,i in enumerate(conf_m)]/rec \n",
    "#     save_dict['classwise_prec_and_rec'] = (precision, recall)\n",
    "    \n",
    "    # micro precision and recall\n",
    "    precision = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(pre) \n",
    "    recall = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(rec)\n",
    "    print(\"micro--\",precision, recall)\n",
    "    save_dict['micro_precision_and_recall'] = (precision, recall)\n",
    "    \n",
    "    # accuracy\n",
    "    acc = len([i for i in range(len(expected)) if expected[i]==predicted[i]])/len(expected)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    save_dict['accuracy'] = acc\n",
    "    \n",
    "    # micro f1 score\n",
    "    f_1 = 2*((precision*recall)/(precision+recall))\n",
    "    print(f\"F1 score: {f_1}\")\n",
    "    save_dict['micro_f1'] = f_1\n",
    "    \n",
    "    exp_results[fold] = save_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_gram', 'pos_tags', 'vocab', 't_prob', 'e_prob', 'true_val', 'pred_val', 'pos_index', 'index_pos', 'labels', 'conf_matrix', 'classwise_prec_and_rec', 'micro_precision_and_recall', 'accuracy', 'micro_f1'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results['fold_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8139534883720931"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results['fold_0']['micro_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exp_results.pk\", \"wb\")as f:\n",
    "    pickle.dump(exp_results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exp_results.pk', 'rb') as f:\n",
    "    exp_results = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# txt = [[(j[1], pos_tags, t_tag, start_prob, transition_probability, emission_probability) for j in i] for i in val]\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# def run(Viterbit, txt):\n",
    "#     with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "#         results = list(tqdm((executor.map(Viterbit, txt)), total=len(txt)))\n",
    "#     return results\n",
    "\n",
    "# print(\"starting the processes\")\n",
    "# temp = run(Viterbit, txt)\n",
    "# print(\"saving the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = {i:e for e,i in enumerate(pos_tags)}\n",
    "index_pos = {e:i for e,i in enumerate(pos_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [pos_index[i] for i in  true]\n",
    "predicted = [pos_index[i] for i in  out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(expected), len(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected = [1, 1, 0,2,2,2,3,3,4,0,0,4,4,4,4,4,4,4,4,7]\n",
    "# predicted = [1, 0, 0,2,1,2,3,3,3,4,3,3,4,3,4,4,4,4,4,6]\n",
    "\n",
    "# # expected = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "# # predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "# # Counter(expected)\n",
    "labels = set(expected+predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = sorted(pos_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eps = 0.1e-200\n",
    "\n",
    "dummy = OrderedDict()\n",
    "for i in labels:\n",
    "    dummy[str(i)] = {str(j) : eps for j in labels}\n",
    "for r, i in enumerate(expected):\n",
    "    dummy[str(i)][str(predicted[r])] +=1\n",
    "    \n",
    "conf_m = np.array([[j for j in i.values()] for i in dummy.values()]) # confusion matrix\n",
    "# dummy, conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = np.sum(conf_m,axis=0)\n",
    "rec = np.sum(conf_m,axis=1)\n",
    "#classwise \n",
    "precision = [i[r] for r,i in enumerate(conf_m)]/pre \n",
    "recall = [i[r] for r,i in enumerate(conf_m)]/rec \n",
    "# print(\"--\",precision, recall)\n",
    "#overall macro\n",
    "print(\"macro--\",sum(precision)/ len(precision), sum(recall)/len(recall))\n",
    "\n",
    "#overall micro\n",
    "precision = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(pre) \n",
    "recall = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(rec)\n",
    "print(\"micro--\",precision, recall)\n",
    "\n",
    "print(f\"Accuracy: {len([i for i in range(len(expected)) if expected[i]==predicted[i]])/len(expected)}\")\n",
    "\n",
    "f\"F1 score: {2*((precision*recall)/(precision+recall))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "results = confusion_matrix(expected, predicted)\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# precision_score(expected, predicted, average =None), recall_score(expected, predicted, average =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(expected, predicted, average =\"macro\"), recall_score(expected, predicted, average =\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(expected, predicted, average =\"weighted\"), recall_score(expected, predicted, average =\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(expected, predicted, average =\"micro\"), recall_score(expected, predicted, average =\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(expected, predicted, average='micro'),f1_score(expected, predicted, average='macro'),f1_score(expected, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

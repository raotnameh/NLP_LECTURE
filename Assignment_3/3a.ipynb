{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-2ibXdz8Oy7"
   },
   "source": [
    "# Assignment 3 | Sentiment Analyisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzPLO7KI8LD3"
   },
   "source": [
    "### Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12049,
     "status": "ok",
     "timestamp": 1604509688223,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "80S_izjA6boo",
    "outputId": "2cf0e55c-455a-434b-8242-a5eea147414f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hemant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hemant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Requirement already satisfied: tweet-preprocessor in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: tweet-preprocessor in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: emosent-py in /opt/anaconda/anaconda3/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied: emosent-py in /opt/anaconda/anaconda3/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied: emoji in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from emosent import get_emoji_sentiment_rank\n",
    "\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#installing tweet-preprocessor\n",
    "from tqdm.auto import tqdm\n",
    "!pip install tweet-preprocessor --user\n",
    "import preprocessor as p\n",
    "from pathlib import Path\n",
    "!pip install emosent-py --user\n",
    "#emoji library\n",
    "!pip install emoji --user\n",
    "import emoji\n",
    "\n",
    "# model save\n",
    "import joblib\n",
    "\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10016,
     "status": "ok",
     "timestamp": 1604509688225,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "2vO89PLYLdSu"
   },
   "outputs": [],
   "source": [
    "class featurise(object):\n",
    "    def __init__(self, x,y):\n",
    "        super(featurise, self).__init__()\n",
    "        self.seq = None\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        ##########################################\n",
    "        self.final_df = pd.DataFrame()\n",
    "        self.final_df['tweets'] = x\n",
    "        self.final_df['target'] = y\n",
    "        ##########################################\n",
    "        self.negation_w = set(['never', 'no', 'nothing', 'nowhere', 'noone', 'none', 'not', 'havent', 'hasnt', 'hadnt', 'cant', \n",
    "                         'couldnt', 'shouldnt', 'wont', 'wouldnt', 'dont', 'doesnt', 'didnt', 'isnt', 'arent', 'aint'] )\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.stop_words.remove('not')\n",
    "        \n",
    "        self.nrc = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/7. NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\", header=None, sep=\"\\t\")\n",
    "        self.dic = {self.nrc[0][i]:self.nrc[1][i] for i in range(len(self.nrc))}\n",
    "        self.mpqa_df = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/mpqa.txt\",header=None, sep=\"\\t\")\n",
    "        self.dic_mpqa = self.mpqa_df.set_index(0).to_dict()\n",
    "        self.bingliu_df = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/1. BingLiu.csv\", header=None, sep=\"\\t\")\n",
    "        self.dic_bing = self.bingliu_df.set_index(0).to_dict()\n",
    "        self.sent140_df = pd.read_csv(\"/home/hemant/NLP_LECTURE/Assignment_3/data/3a/lexicons/3. Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\", header=None, sep=\"\\t\")\n",
    "        self.dic_sent = self.sent140_df.set_index(0).to_dict()\n",
    "\n",
    "    def elongated_word(self, x):\n",
    "        count = 0\n",
    "        regex_ = re.compile(r\"(.)\\1{2}\")\n",
    "        for i in x.split():            \n",
    "            if regex_.search(i): count+=1\n",
    "        return count\n",
    "        \n",
    "    def emoji(self, x):\n",
    "        emo_final = []\n",
    "        for word_ in tqdm(x):\n",
    "            emoji_ = {d:word for d,word in enumerate(regex.findall(r'\\X', word_)) if any(char in emoji.UNICODE_EMOJI for char in word)}\n",
    "\n",
    "            dummy = [0,0,0]\n",
    "            for i,e in emoji_.items():\n",
    "                if i == len(word_) - 1:\n",
    "                    dummy[-1] += 1\n",
    "                    continue\n",
    "                elif get_emoji_sentiment_rank(e)['positive'] > get_emoji_sentiment_rank(e)['negative']: dummy[0] += 1\n",
    "                elif get_emoji_sentiment_rank(e)['positive'] < get_emoji_sentiment_rank(e)['negative']: dummy[1] += 1\n",
    "            emo_final.append(dummy)\n",
    "        emo_final = np.array(emo_final)\n",
    "        self.final_df['p+'] = emo_final[:,0]\n",
    "        self.final_df['p-'] = emo_final[:,1]\n",
    "        self.final_df['p+-'] = emo_final[:,2]\n",
    "    \n",
    "    def punctuation(self, x):\n",
    "        regex = re.compile(r\"([?|!])\\1{0}\")\n",
    "        dummy = []\n",
    "        for i in tqdm(x):\n",
    "            d = [len([word for word in i.split() if regex.search(word)])]\n",
    "            if regex.search(i.split()[-1]): d.append(1)\n",
    "            else: d.append(0)\n",
    "            dummy.append(d)\n",
    "        dummy = np.array(dummy)\n",
    "        self.final_df['punctu_1'] = dummy[:,0]\n",
    "        self.final_df['punctu_2'] = dummy[:,1]\n",
    "                   \n",
    "    def all_caps(self, x):\n",
    "        return len([1 for each in x.split() if each.isupper()])\n",
    "\n",
    "    def hashtag_count(self, x):\n",
    "        return len([1 for word in x.split() if word.startswith('#') and len(word)>1 ])\n",
    "    \n",
    "    def negation(self, x):\n",
    "        flag = 0\n",
    "        splits = \",.:;!?\"\n",
    "        final = []\n",
    "        count = 0\n",
    "#         x = f\"I don't think I will enjoy it: it might be too spicy.\".lower()\n",
    "        for word in x.split():\n",
    "            if flag ==1:\n",
    "                for symb in splits:\n",
    "                    if symb in word:\n",
    "                        flag = 0\n",
    "                        word_split = word.split(symb)\n",
    "                        \n",
    "                        if len(word_split[0])>0:\n",
    "                        \n",
    "                            count+=1\n",
    "                        \n",
    "                            if len(word_split[-1])>1:\n",
    "                        \n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                if flag==1:\n",
    "                    \n",
    "                    count+=1\n",
    "                \n",
    "            elif flag==0:\n",
    "                \n",
    "                pass\n",
    "\n",
    "            clean = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "            if clean in self.negation_w:\n",
    "                flag=1\n",
    "        return count\n",
    "\n",
    "    def pos_tag_feat(self, a):\n",
    "        print('pos_tags')\n",
    "        tagged_texts = [pos_tag_sents([i.split()])[0] for i in tqdm(a)]\n",
    "\n",
    "        dummy = [Counter([j[-1] for j in i]) for i in tqdm(tagged_texts)]\n",
    "        pos_vocab = set([j for i in dummy for j in i.keys()])\n",
    "        pos_t_i = {i:d for d,i in enumerate(pos_vocab)}\n",
    "        a = np.zeros((len(tagged_texts),len(pos_vocab)))\n",
    "\n",
    "        for d,i in tqdm(enumerate(dummy)):\n",
    "            for k,v in i.items():\n",
    "                a[d][pos_t_i[k]] = v\n",
    "        for d,i in enumerate(pos_t_i.keys()):\n",
    "            self.final_df[i] = a[:,d]\n",
    "#         return pd.DataFrame(a,columns=pos_t_i.keys())\n",
    "\n",
    "    #########################\n",
    "    \n",
    "    def remove_stopword(self, x):\n",
    "        dummy = []\n",
    "        for i in x.split():\n",
    "            if i not in self.stop_words: dummy.append(i)\n",
    "        return ' '.join(dummy)\n",
    "    \n",
    "    def all_lower(self, x):\n",
    "        return x.lower()\n",
    "    def remove_punctuation(self, x):\n",
    "        \n",
    "        return re.sub(r'[^\\w\\s]', '', x)\n",
    "    def remove_number(self, x):\n",
    "        return ''.join([_ for _ in x if not _.isdigit()])\n",
    "    def rem_un_url(self, x):\n",
    "        # return p.clean(x)\n",
    "        result = re.sub(r\"http\\S+\", \"\", x)\n",
    "        return re.sub('@[^\\s]+','',result)\n",
    "\n",
    "    #################################################################3\n",
    "    def nrc_emotion_score(self,x):\n",
    "\n",
    "        score = 0\n",
    "        for each in  x.lower().split():\n",
    "            if each in self.dic: score += self.dic[each]\n",
    "        return score\n",
    "    \n",
    "    def polar_word_count_mpqa(self, x):\n",
    "\n",
    "        pos,neg = 0,0\n",
    "        for each in  x.split():\n",
    "            if each.lower() in self.dic_mpqa[1]:\n",
    "                if self.dic_mpqa[1][each.lower()] == 'negative': neg+=1\n",
    "                else: pos+=1\n",
    "        return pos,neg\n",
    "    \n",
    "    def polar_word_count_bingliu(self, x):\n",
    "        pos,neg = 0,0\n",
    "        for each in  x.split():\n",
    "            if each.lower() in self.dic_bing[1]:\n",
    "                if self.dic_bing[1][each.lower()] == 'negative': neg+=1\n",
    "                else: pos+=1\n",
    "        return pos,neg\n",
    "\n",
    "    \n",
    "    def agg_polarity_score_sent140(self,x):\n",
    "        \n",
    "        pos,neg = 0,0\n",
    "        for each in  x.lower().split():\n",
    "            if each in self.dic_sent[2]:\n",
    "                pos+=self.dic_sent[2][each]\n",
    "            if each in self.dic_sent[3]:\n",
    "                neg+=self.dic_sent[3][each]\n",
    "        return pos, neg\n",
    "    \n",
    "    def process(self):\n",
    "        self.final_df['elongated'] = tqdm(self.x.map(self.elongated_word))\n",
    "        self.emoji(list(self.x))\n",
    "        self.punctuation(list(self.x))\n",
    "\n",
    "        self.final_df['allcaps'] = [self.all_caps(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['hashtag'] = [self.hashtag_count(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['negation'] = [self.negation(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['negation'] = [self.negation(i)  for i in tqdm(self.x)]\n",
    "        \n",
    "        print('shit')\n",
    "        self.final_df['nrc_emotion_score'] = [self.nrc_emotion_score(i) for i in tqdm(self.x)]\n",
    "        dummy = np.array([self.polar_word_count_mpqa(i) for i in tqdm(self.x)])\n",
    "        self.final_df[\"pos_mpqa\"] = dummy[:,0]\n",
    "        self.final_df[\"neg_mpqa\"] = dummy[:,1]\n",
    "        dummy = np.array([self.polar_word_count_bingliu(i) for i in tqdm(self.x)])\n",
    "        self.final_df['pos_bilingu'] = dummy[:,0]\n",
    "        self.final_df['neg_bilingu'] = dummy[:,1]\n",
    "        dummy = np.array([self.agg_polarity_score_sent140(i) for i in tqdm(self.x)])\n",
    "        self.final_df['pos_sent140'] = dummy[:,0]\n",
    "        self.final_df['neg_sent140'] = dummy[:,1]\n",
    "\n",
    "        \n",
    "        self.final_df['new_tweets'] = [self.rem_un_url(i)  for i in tqdm(self.x)]\n",
    "        self.final_df['new_tweets'] = [self.all_lower(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        self.final_df['new_tweets'] = [self.remove_number(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        \n",
    "        self.pos_tag_feat(list(self.final_df['new_tweets']))\n",
    "        \n",
    "        self.final_df['new_tweets'] = [self.remove_punctuation(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        print(\"shit\")\n",
    "        self.final_df['new_tweets'] = [self.remove_stopword(i) for i in tqdm(self.final_df['new_tweets'])]\n",
    "        return self\n",
    "\n",
    "################################################################################\n",
    "def printdf(df, rows = 5):\n",
    "    print(df[:rows].to_markdown())\n",
    "    return True \n",
    "  \n",
    "#unigrams bow\n",
    "def bow(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "\n",
    "    count_vect = CountVectorizer() #in scikit-learn\n",
    "    count_vect.fit(X_train)\n",
    "\n",
    "    bow_counts_train = count_vect.transform(X_train)\n",
    "    bow_counts_cv = count_vect.transform(X_cv)\n",
    "    bow_counts_test = count_vect.transform(X_test)\n",
    "    print(\"the shape of out text BOW vectorizer \",bow_counts_train.get_shape())\n",
    "    print(\"the number of unique words \", bow_counts_train.get_shape()[1])\n",
    "    # final_counts_test=final_counts_test.toarray()\n",
    "    return count_vect, bow_counts_train, bow_counts_cv, bow_counts_test\n",
    "\n",
    "#bigrams bow\n",
    "def bigram(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "    count_vect = CountVectorizer(ngram_range=(2,2))\n",
    "    bigram_counts_train = count_vect.fit_transform(X_train)\n",
    "    bigram_counts_cv = count_vect.transform(X_cv)\n",
    "    bigram_counts_test = count_vect.transform(X_test)\n",
    "    print(\"some feature names \", count_vect.get_feature_names()[:10])\n",
    "    print(\"the shape of out  BIGRAM vectorizer \",bigram_counts_train.get_shape())\n",
    "    print(\"the number of unique words with bigrams \", bigram_counts_train.get_shape()[1])\n",
    "    return count_vect, bigram_counts_train, bigram_counts_cv, bigram_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6820,
     "status": "ok",
     "timestamp": 1604509829782,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "FtGOG6pM7nKC",
    "outputId": "28df25b8-3d3d-4409-b6e4-745416caf55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120000, 6)\n",
      "(480000, 6)\n",
      "(1120000, 6)\n",
      "(480000, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/3a/sentiment_train.csv\")\n",
    "test = pd.read_csv(\"data/3a/sentiment_test.csv\")\n",
    "\n",
    "# uncomment this later \n",
    "# train = train[:30000]\n",
    "# test = test[:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"5\"]\n",
    "y_train = train[\"0\"]\n",
    "X_test = test[\"5\"]\n",
    "y_test = test['0']\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27228,
     "status": "ok",
     "timestamp": 1604509710650,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "Qwf3CBZ223dJ",
    "outputId": "2479cf50-9ab4-4b3c-8cfc-e14b087555ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Found\n",
      "CSV Found\n",
      "CSV Found\n",
      "CSV Found\n",
      "CSV Found\n",
      "CSV Found\n"
     ]
    }
   ],
   "source": [
    "#### FEATURISATION\n",
    "\n",
    "my_file = Path(\"data/3a/train_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    train_df = pd.read_csv(\"data/3a/train_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise train data\n",
    "    proc = featurise(X_train,y_train).process()\n",
    "    train_df = proc.final_df\n",
    "    train_df.to_csv('data/3a/train_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/val_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    val_df = pd.read_csv(\"data/3a/val_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise cv data\n",
    "    proc = featurise(X_cv,y_cv).process()\n",
    "    val_df = proc.final_df\n",
    "    val_df.to_csv('data/3a/val_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/test_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    test_df = pd.read_csv(\"data/3a/test_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise test data\n",
    "    proc = featurise(X_test,y_test).process()\n",
    "    test_df = proc.final_df\n",
    "    test_df.to_csv('data/3a/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 537824,
     "status": "ok",
     "timestamp": 1604498381196,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "sFYM9N42oFnK"
   },
   "outputs": [],
   "source": [
    "# train_df.isnull().values.any()\n",
    "# train_df['new_tweets'].isnull().sum()\n",
    "# df['your column name'].isnull().sum()\n",
    "train_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "val_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "test_df.dropna(subset = [\"new_tweets\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593820,
     "status": "ok",
     "timestamp": 1604498441480,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "d8kHJBcC34W_",
    "outputId": "298c059a-e285-4272-9e4e-36d1ee8c0744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of out text BOW vectorizer  (747244, 253751)\n",
      "the number of unique words  253751\n",
      "the shape of out text BOW vectorizer  (747244, 253751)\n",
      "the number of unique words  253751\n"
     ]
    }
   ],
   "source": [
    "count_vect, bow_counts_train, bow_counts_cv, bow_counts_test = bow(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])\n",
    "# count_vect_, bigram_counts_train, bigram_counts_cv, bigram_counts_test = bigram(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bow_counts_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c8b6436f0e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_counts_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bow_counts_cv' is not defined"
     ]
    }
   ],
   "source": [
    "bow_counts_cv[:10000].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 610735,
     "status": "ok",
     "timestamp": 1604498462093,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "1u2OIERBS07X"
   },
   "outputs": [],
   "source": [
    "# from scipy import sparse\n",
    "# def save_npz(f,name):\n",
    "#     sparse.save_npz(\"data/3a/npz/\"+str(name)+\".npz\", f)\n",
    "#     return True\n",
    "\n",
    "# save_npz(bow_counts_cv, \"bow_counts_cv\")\n",
    "# save_npz(bow_counts_test, \"bow_counts_test\")\n",
    "# save_npz(bow_counts_train, \"bow_counts_train\")\n",
    "# save_npz(bigram_counts_train, \"bigram_counts_train\")\n",
    "# save_npz(bigram_counts_test, \"bigram_counts_test\")\n",
    "# save_npz(bigram_counts_cv, \"bigram_counts_cv\")\n",
    "\n",
    "# # train_df.to_csv('data/3a/gram_traincsv.csv')\n",
    "# # val_df.to_csv('data/3a/gram_valcsv.csv')\n",
    "# # test_df.to_csv('data/3a/gram_testcsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1604491907199,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "XEkHXz3XCCMb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1604493558672,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "rJvJUSIwJDmX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2FMJ3VHPU0ttKEQbYUYon",
   "collapsed_sections": [],
   "mount_file_id": "1rSUMnud92JcJuJyZ5Rh2ITMs9pKt7nE3",
   "name": "3a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

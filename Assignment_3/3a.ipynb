{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-2ibXdz8Oy7"
   },
   "source": [
    "# Assignment 3 | Sentiment Analyisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzPLO7KI8LD3"
   },
   "source": [
    "### Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12049,
     "status": "ok",
     "timestamp": 1604509688223,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "80S_izjA6boo",
    "outputId": "2cf0e55c-455a-434b-8242-a5eea147414f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hemant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Requirement already satisfied: tweet-preprocessor in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#installing tweet-preprocessor\n",
    "from tqdm.auto import tqdm\n",
    "!pip install tweet-preprocessor --user\n",
    "import preprocessor as p\n",
    "from pathlib import Path\n",
    "\n",
    "#emoji library\n",
    "!pip install emoji --user\n",
    "import emoji\n",
    "\n",
    "# model save\n",
    "import joblib\n",
    "\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = set(['never', 'no', 'nothing', 'nowhere', 'noone', 'none', 'not', 'havent', 'hasnt', 'hadnt', 'cant', \n",
    "                         'couldnt', 'shouldnt', 'wont', 'wouldnt', 'dont', 'doesnt', 'didnt', 'isnt', 'arent', 'aint'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list('sdfnds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lits = ['.', ',', ':', ';', '!', '?']\n",
    "count = 0\n",
    "c = -1\n",
    "x = f\"I don't think I will enjoy it: it might be too spicy.\".lower()\n",
    "for w in x.split():\n",
    "    if re.sub(r'[^\\w\\s]', '', w) in w_ or c > -1:\n",
    "        c+=1\n",
    "    if w[-1] in splits:\n",
    "        count += c\n",
    "        c = -1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation(self, x):\n",
    "    splits = ['.', ',', ':', ';', '!', '?']\n",
    "    count = 0\n",
    "    c = -1\n",
    "    x = f\"I don't think I will enjoy it: it might be too spicy.\".lower()\n",
    "    for w in x.split():\n",
    "        if w in w_ or c > -1:\n",
    "            print(w)\n",
    "            c+=1\n",
    "        if w[-1] in splits:\n",
    "            count += c\n",
    "            c = -1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 10016,
     "status": "ok",
     "timestamp": 1604509688225,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "2vO89PLYLdSu"
   },
   "outputs": [],
   "source": [
    "class featurise(object):\n",
    "    def __init__(self, x,y):\n",
    "        super(featurise, self).__init__()\n",
    "        self.seq = None\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        ##########################################\n",
    "        self.final_df = pd.DataFrame()\n",
    "        self.final_df['tweets'] = x\n",
    "        self.final_df['target'] = y\n",
    "        ##########################################\n",
    "        self.negation_w = set(['never', 'no', 'nothing', 'nowhere', 'noone', 'none', 'not', 'havent', 'hasnt', 'hadnt', 'cant', \n",
    "                         'couldnt', 'shouldnt', 'wont', 'wouldnt', 'dont', 'doesnt', 'didnt', 'isnt', 'arent', 'aint'] )\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.stop_words.remove('not')\n",
    "\n",
    "    def elongated_word(self, x):\n",
    "        regex_ = re.compile(r\"(.)\\1{2}\")\n",
    "        if regex_.search(x): return True\n",
    "        else: return False\n",
    "\n",
    "    def emoji(self, x): # Doubt\n",
    "        return len([1 for word in regex.findall(r'\\X', x) if any(char in emoji.UNICODE_EMOJI for char in word)])\n",
    "    \n",
    "    def punctuation(self, x):\n",
    "        punct_ = string.punctuation\n",
    "        return len([1 for w in x if w in punct_])\n",
    "    \n",
    "    def all_caps(self, x):\n",
    "        return len([1 for each in x if each.isupper()])\n",
    "\n",
    "    def hashtag_count(self, x):\n",
    "        return len([word for word in x.split() if word.startswith('#') and len(word)>1 ])\n",
    "    \n",
    "    def negation(self, x):\n",
    "        flag = 0\n",
    "        splits = \",.:;!?\"\n",
    "        final = []\n",
    "        count = 0\n",
    "#         x = f\"I don't think I will enjoy it: it might be too spicy.\".lower()\n",
    "        for word in x.split():\n",
    "            if flag ==1:\n",
    "                for symb in splits:\n",
    "                    if symb in word:\n",
    "                        flag = 0\n",
    "                        word_split = word.split(symb)\n",
    "                        \n",
    "                        if len(word_split[0])>0:\n",
    "                        \n",
    "                            count+=1\n",
    "                        \n",
    "                            if len(word_split[-1])>1:\n",
    "                        \n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                if flag==1:\n",
    "                    \n",
    "                    count+=1\n",
    "                \n",
    "            elif flag==0:\n",
    "                \n",
    "                pass\n",
    "\n",
    "            clean = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "            if clean in self.negation_w:\n",
    "                flag=1\n",
    "        return count\n",
    "\n",
    "    def pos_tag_feat(self, a):\n",
    "        tagged_texts = pos_tag_sents(map(word_tokenize, a))\n",
    "\n",
    "        df = pd.DataFrame(a)\n",
    "        df['pos'] = tagged_texts\n",
    "        pos_vocab=set()\n",
    "        for pos in tagged_texts:\n",
    "            for each in pos:\n",
    "                pos_vocab.add(each[1])\n",
    "        def pos_count(pos_sent, pos_vocab):\n",
    "            pos_feature = [0]* len(pos_vocab)\n",
    "            for each in pos_sent:\n",
    "                pos_feature[pos_vocab.index(each[1])] +=1\n",
    "            return pos_feature\n",
    "        pos_feature_df = []\n",
    "        for i in df['pos']:\n",
    "            pos_feature_df.append(pos_count(i,list(pos_vocab)))\n",
    "        df = pd.DataFrame(pos_feature_df, columns=list(pos_vocab))\n",
    "        return df\n",
    "\n",
    "    #########################\n",
    "    \n",
    "    def remove_stopword(self, x):\n",
    "        dummy = []\n",
    "        for i in x.split():\n",
    "            if i not in self.stop_words: dummy.append(i)\n",
    "        return ' '.join(dummy)\n",
    "    \n",
    "    def all_lower(self, x):\n",
    "        return x.lower()\n",
    "    def remove_punctuation(self, x):\n",
    "        \n",
    "        return re.sub(r'[^\\w\\s]', '', x)\n",
    "    def remove_number(self, x):\n",
    "        return ''.join([_ for _ in x if not _.isdigit()])\n",
    "    def rem_un_url(self, x):\n",
    "        return p.clean(x)\n",
    "        \n",
    "    def process(self):\n",
    "        self.final_df['elogated'] = tqdm(self.x.map(self.elongated_word))\n",
    "        self.final_df['emoji'] = tqdm(self.x.map(self.emoji))\n",
    "        self.final_df['punctuation'] = tqdm(self.x.map(self.punctuation))\n",
    "        self.final_df['allcaps'] = tqdm(self.x.map(self.all_caps))\n",
    "        self.final_df['hashtag'] = tqdm(self.x.map(self.hashtag_count))\n",
    "        self.final_df['negation'] = tqdm(self.x.map(self.negation))\n",
    "        self.final_df['negation'] = tqdm(self.x.map(self.negation))\n",
    "        \n",
    "        self.final_df['new_tweets'] = tqdm(self.x.map(self.rem_un_url))\n",
    "        self.final_df['new_tweets'] = tqdm(self.final_df['new_tweets'].map(self.all_lower))\n",
    "        self.final_df['new_tweets'] = tqdm(self.final_df['new_tweets'].map(self.remove_number))\n",
    "        \n",
    "        new_df = self.pos_tag_feat(list(self.final_df['new_tweets']))\n",
    "        self.final_df = pd.concat([self.final_df, new_df], axis=1)\n",
    "        \n",
    "        self.final_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "        self.final_df['new_tweets'] = self.final_df['new_tweets'].map(self.remove_punctuation)\n",
    "        \n",
    "        self.final_df['new_tweets'] = self.final_df['new_tweets'].map(self.remove_stopword)\n",
    "        return self\n",
    "\n",
    "################################################################################\n",
    "def printdf(df, rows = 5):\n",
    "    print(df[:rows].to_markdown())\n",
    "    return True \n",
    "  \n",
    "#unigrams bow\n",
    "def bow(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "\n",
    "    count_vect = CountVectorizer() #in scikit-learn\n",
    "    count_vect.fit(X_train)\n",
    "\n",
    "    bow_counts_train = count_vect.transform(X_train)\n",
    "    bow_counts_cv = count_vect.transform(X_cv)\n",
    "    bow_counts_test = count_vect.transform(X_test)\n",
    "    print(\"the shape of out text BOW vectorizer \",bow_counts_train.get_shape())\n",
    "    print(\"the number of unique words \", bow_counts_train.get_shape()[1])\n",
    "    # final_counts_test=final_counts_test.toarray()\n",
    "    return bow_counts_train, bow_counts_cv, bow_counts_test\n",
    "\n",
    "#bigrams bow\n",
    "def bigram(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "    count_vect = CountVectorizer(ngram_range=(2,2))\n",
    "    bigram_counts_train = count_vect.fit_transform(X_train)\n",
    "    bigram_counts_cv = count_vect.transform(X_cv)\n",
    "    bigram_counts_test = count_vect.transform(X_test)\n",
    "    print(\"some feature names \", count_vect.get_feature_names()[:10])\n",
    "    print(\"the shape of out  BIGRAM vectorizer \",bigram_counts_train.get_shape())\n",
    "    print(\"the number of unique words with bigrams \", bigram_counts_train.get_shape()[1])\n",
    "    return bigram_counts_train, bigram_counts_cv, bigram_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6820,
     "status": "ok",
     "timestamp": 1604509829782,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "FtGOG6pM7nKC",
    "outputId": "28df25b8-3d3d-4409-b6e4-745416caf55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6)\n",
      "(200, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/3a/sentiment_train.csv\")\n",
    "test = pd.read_csv(\"data/3a/sentiment_test.csv\")\n",
    "\n",
    "# uncomment this later \n",
    "train = train[:300]\n",
    "test = test[:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27228,
     "status": "ok",
     "timestamp": 1604509710650,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "Qwf3CBZ223dJ",
    "outputId": "2479cf50-9ab4-4b3c-8cfc-e14b087555ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CSV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be84d050434b433da554e64f4e27d646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca62ec873704845aaf7ab55f7d8e776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e1eea803e546dcb3138ad300d156db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809c010bbfae458eacd8f7f96bf6d288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f1b81cbcb2400780ec753f37e0d55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c04d23dcc140c9ac7781df810130fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c50a25a5ce4a8483460a7c69934395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcc2b0dcec2420f9ad9b105c7424651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33f1011ed624386ba2a40c73aba7b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186cb043d9fe4a9caacfbb62d78fa7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CSV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dabef2afe143599274af1f38953d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331f38125def45389933ac2c0ac35d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a44e2a088e45588b99ded96f7709fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bafbdb5654498db7cf3bb2d7133c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0ac665b36f454d909fbf367aed9b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5167d28131304e3f9b9dd764c18a6346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e1433f535546b88649b61ae1e20898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3760db7cf1940e58054745423dd091e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d78dde24e6446da91534280f4e8ed9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26ccbc7e39c4a6aa17ed6535122fb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CSV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2532051e34d84ab39ed1a372c2f8d7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfe8d18f5de47f19d219c02d116055e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd81d4a9e52f45fd9105ffb259c8b792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dacdc5aaff4738ad337c9ae0ae3fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f6b2899044ca0a8433e86626035e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a78ff60c0c84726b4588d06076209b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebf1329035c4b3ba97481058c5b4b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0014abcfe1354f2094b51c258e583807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861d0103416147e19f6ebdcf657fb379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac41322e60d4449fa8809fcc243310e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = train[\"5\"]\n",
    "y_train = train[\"0\"]\n",
    "X_test = test[\"5\"]\n",
    "y_test = test['0']\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#### FEATURISATION\n",
    "\n",
    "my_file = Path(\"data/3a/train_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    train_df = pd.read_csv(\"data/3a/train_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise train data\n",
    "    proc = featurise(X_train,y_train).process()\n",
    "    train_df = proc.final_df\n",
    "    train_df.to_csv('data/3a/train_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/val_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    val_df = pd.read_csv(\"data/3a/val_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise cv data\n",
    "    proc = featurise(X_cv,y_cv).process()\n",
    "    val_df = proc.final_df\n",
    "    val_df.to_csv('data/3a/val_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/test_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    test_df = pd.read_csv(\"data/3a/test_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise test data\n",
    "    proc = featurise(X_test,y_test).process()\n",
    "    test_df = proc.final_df\n",
    "    test_df.to_csv('data/3a/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1604509914004,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "UuPyLJzzVuvO",
    "outputId": "3526942d-c972-48e3-db6e-cab61da97c22"
   },
   "outputs": [],
   "source": [
    "test_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 537824,
     "status": "ok",
     "timestamp": 1604498381196,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "sFYM9N42oFnK"
   },
   "outputs": [],
   "source": [
    "# train_df.isnull().values.any()\n",
    "# train_df['new_tweets'].isnull().sum()\n",
    "# df['your column name'].isnull().sum()\n",
    "train_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "val_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "test_df.dropna(subset = [\"new_tweets\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593820,
     "status": "ok",
     "timestamp": 1604498441480,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "d8kHJBcC34W_",
    "outputId": "298c059a-e285-4272-9e4e-36d1ee8c0744"
   },
   "outputs": [],
   "source": [
    "bow_counts_train, bow_counts_cv, bow_counts_test = bow(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])\n",
    "bigram_counts_train, bigram_counts_cv, bigram_counts_test = bigram(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 610735,
     "status": "ok",
     "timestamp": 1604498462093,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "1u2OIERBS07X"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def save_npz(f,name):\n",
    "    sparse.save_npz(\"/content/drive/My Drive/IIITD Data/NLP/a3_data/npz/\"+str(name)+\".npz\", f)\n",
    "    return True\n",
    "\n",
    "save_npz(bow_counts_cv, \"bow_counts_cv\")\n",
    "save_npz(bow_counts_test, \"bow_counts_test\")\n",
    "save_npz(bow_counts_train, \"bow_counts_train\")\n",
    "save_npz(bigram_counts_train, \"bigram_counts_train\")\n",
    "save_npz(bigram_counts_test, \"bigram_counts_test\")\n",
    "save_npz(bigram_counts_cv, \"bigram_counts_cv\")\n",
    "\n",
    "train_df.to_csv('/content/drive/My Drive/IIITD Data/NLP/a3_data/gram_traincsv.csv')\n",
    "val_df.to_csv('/content/drive/My Drive/IIITD Data/NLP/a3_data/gram_valcsv.csv')\n",
    "test_df.to_csv('/content/drive/My Drive/IIITD Data/NLP/a3_data/gram_testcsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 1703,
     "status": "ok",
     "timestamp": 1604493537242,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "GqmfVZTUCoV2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IN</th>\n",
       "      <th>PRP</th>\n",
       "      <th>TO</th>\n",
       "      <th>WRB</th>\n",
       "      <th>RB</th>\n",
       "      <th>NNP</th>\n",
       "      <th>VBP</th>\n",
       "      <th>MD</th>\n",
       "      <th>NN</th>\n",
       "      <th>WDT</th>\n",
       "      <th>NNS</th>\n",
       "      <th>DT</th>\n",
       "      <th>.</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>VBN</th>\n",
       "      <th>CD</th>\n",
       "      <th>JJ</th>\n",
       "      <th>VB</th>\n",
       "      <th>VBG</th>\n",
       "      <th>CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IN  PRP  TO  WRB  RB  NNP  VBP  MD  NN  WDT  NNS  DT  .  VBZ  VBN  CD  JJ  \\\n",
       "0   8    1   1    1   0    2    1   0   8    0    2   4  0    0    1   1   1   \n",
       "1   2    0   1    0   2    0    0   1   2    1    0   2  0    1    0   0   1   \n",
       "2   2    0   0    0   0    0    0   0   0    0    1   1  0    0    1   0   0   \n",
       "3   1    0   1    1   1    0    0   0   4    0    0   4  1    1    0   0   0   \n",
       "\n",
       "   VB  VBG  CC  \n",
       "0   1    0   0  \n",
       "1   2    1   1  \n",
       "2   0    0   0  \n",
       "3   2    0   2  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = df['Text'].tolist()\n",
    "a= ['In situations where you wish to POS tag a column of text stored in a pandas dataframe with 1 sentence per row the majority of implementations on SO use the apply method',\n",
    "    'Does that apply to this example and if so would the code be as simple as changing',\n",
    "    'As mentioned in the comments',\n",
    "    'but the issue is how to do this and still produce a column in a pandas dataframe?']\n",
    "# comment the above portion and uncomment first line to take all tweets to list format\n",
    "def pos_tag_feat(a):\n",
    "    tagged_texts = pos_tag_sents(map(word_tokenize, a))\n",
    "\n",
    "    df = pd.DataFrame(a)\n",
    "    df['pos'] = tagged_texts\n",
    "    pos_vocab=set()\n",
    "    for pos in tagged_texts:\n",
    "        for each in pos:\n",
    "            pos_vocab.add(each[1])\n",
    "    def pos_count(pos_sent, pos_vocab):\n",
    "        pos_feature = [0]* len(pos_vocab)\n",
    "        for each in pos_sent:\n",
    "            pos_feature[pos_vocab.index(each[1])] +=1\n",
    "        return pos_feature\n",
    "    pos_feature_df = []\n",
    "    for i in df['pos']:\n",
    "        pos_feature_df.append(pos_count(i,list(pos_vocab)))\n",
    "    df = pd.DataFrame(pos_feature_df, columns=list(pos_vocab))\n",
    "    return df\n",
    "pos_tag_feat(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epiGbEKkKLOt"
   },
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IN</th>\n",
       "      <th>PRP</th>\n",
       "      <th>TO</th>\n",
       "      <th>WRB</th>\n",
       "      <th>RB</th>\n",
       "      <th>NNP</th>\n",
       "      <th>VBP</th>\n",
       "      <th>MD</th>\n",
       "      <th>NN</th>\n",
       "      <th>WDT</th>\n",
       "      <th>...</th>\n",
       "      <th>NNS</th>\n",
       "      <th>DT</th>\n",
       "      <th>.</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>VBN</th>\n",
       "      <th>CD</th>\n",
       "      <th>JJ</th>\n",
       "      <th>VB</th>\n",
       "      <th>VBG</th>\n",
       "      <th>CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IN  PRP  TO  WRB  RB  NNP  VBP  MD  NN  WDT  ...  NNS  DT  .  VBZ  VBN  CD  \\\n",
       "0   8    1   1    1   0    2    1   0   8    0  ...    2   4  0    0    1   1   \n",
       "1   2    0   1    0   2    0    0   1   2    1  ...    0   2  0    1    0   0   \n",
       "2   2    0   0    0   0    0    0   0   0    0  ...    1   1  0    0    1   0   \n",
       "3   1    0   1    1   1    0    0   0   4    0  ...    0   4  1    1    0   0   \n",
       "\n",
       "   JJ  VB  VBG  CC  \n",
       "0   1   1    0   0  \n",
       "1   1   2    1   1  \n",
       "2   0   0    0   0  \n",
       "3   0   2    0   2  \n",
       "\n",
       "[4 rows x 40 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pos_tag_feat(a),pos_tag_feat(a)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1531,
     "status": "ok",
     "timestamp": 1604493544914,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "ZHbP2Q3uRDdS"
   },
   "outputs": [],
   "source": [
    "# emoji = \"🤔 🙈 me así, se 😌 ds 💕👭👙 hello 👩🏾‍🎓 #emoji hello 👨‍👩‍👦‍👦 how# #are 😊 # ###you today🙅🏽🙅🏽\"\n",
    "# neg = \"never going . to ain't be: th;e pussy, shouldn't be like doesn't is'nt?\"\n",
    "# general = \"This is a sample sentence, showing off the stop words filtration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1604482887508,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "h37VjVEvouNy"
   },
   "outputs": [],
   "source": [
    "!pip install --user -U nltk==3.2.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1885,
     "status": "ok",
     "timestamp": 1604493547664,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "T59nCIyS14ji"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.2.5.\n",
      "The scikit-learn version is 0.23.2.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1604493547667,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "LwNwc5qG1_xh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1604493550961,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "3TQXSf0h5K71"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1660,
     "status": "ok",
     "timestamp": 1604493554570,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "yY83xcc98PeP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1604491907199,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "XEkHXz3XCCMb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1604493558672,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "rJvJUSIwJDmX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2FMJ3VHPU0ttKEQbYUYon",
   "collapsed_sections": [],
   "mount_file_id": "1rSUMnud92JcJuJyZ5Rh2ITMs9pKt7nE3",
   "name": "3a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

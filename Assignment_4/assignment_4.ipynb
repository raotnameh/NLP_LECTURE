{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json, glob, os\n",
    "import pandas as pd\n",
    "import re, string\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = read_('data/english-hindi-dictionary.txt')\n",
    "eng = read_('data/english.txt')\n",
    "hindi = read_('data/hindi.txt')\n",
    "out = read_('data/BingLiu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = { i.split('|||')[0].strip():i.split('|||')[-1].strip() for i in lex if not re.search('[a-zA-Z]', i.split('|||')[-1].strip()) }\n",
    "# lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving for glove \n",
    "# a = ''\n",
    "# for i in eng:\n",
    "#     a+=i.strip().lower() + ' '\n",
    "    \n",
    "# with open('GloVe/eng.txt', 'w') as f:\n",
    "#     f.write(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_word1, hindi_word1, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [[j.strip() for j in i.split('\\t')] for i in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ee56eb2cc043ecb1b2723b12e5cd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6789.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "L1 = ''\n",
    "for i in tqdm(out):\n",
    "    try: L1 += f\"{remove_punctuation(i[0])},{remove_punctuation(lex[i[0]])},{i[-1]}\\n\"\n",
    "    except: pass #L1 += f\"{i[0]},{None},{i[-1]}\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save/L1.txt', 'w') as f:\n",
    "    f.write(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2115\n"
     ]
    }
   ],
   "source": [
    "print(len(L1.split('\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_ = [[j.lower() for j in i.split()]for i in eng]\n",
    "# hindi_ = [[j.lower() for j in i.split()]for i in hindi]\n",
    "\n",
    "# w2v_eng = Word2Vec(eng_, size=100, window=5, min_count=1, workers=48, iter=10)\n",
    "# w2v_hindi = Word2Vec(hindi_, size=100, window=5, min_count=1, workers=48, iter=10)\n",
    "# w2v_eng.save(\"save/w2v_eng.model\")\n",
    "# w2v_hindi.save(\"save/w2v_hindi.model\")\n",
    "w2v_eng = Word2Vec.load(\"save/w2v_eng.model\")\n",
    "w2v_hindi = Word2Vec.load(\"save/w2v_hindi.model\")\n",
    "# model.most_similar('used')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_(d):\n",
    "    return [float(i) for i in d]\n",
    "def glove(path):\n",
    "    return { i.split()[0]:i.split()[1:] for i in read_(path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8074, 10354)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_eng = glove('save/eng/vectors.txt')\n",
    "glove_hindi = glove('save/hindi/vectors.txt')\n",
    "len(glove_eng), len(glove_hindi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e7abf14d8e49aeb48bee79717fa57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2115.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-fc35c83a6ca5>:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  closest[i].append([i[0] for i in w2v_eng.most_similar(i.split(',')[0])][:n])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words matched:- 653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a87d6e499744f04a511be6897763a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2115.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-fc35c83a6ca5>:21: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  closest[i].append([i[0] for i in w2v_hindi.most_similar(i.split(',')[1])][:n])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words matched:- 662\n",
      "['स्थल', 'जीवन', 'तुरंत', 'स्टेशन', 'karbonn'] life जीवन\n",
      "['बड़ा', 'जितना', 'होना', 'रहती', 'सरल'] big बड़ा\n",
      "['काफी', 'अच्छी', 'बहुत', 'बेहतर', 'क्वालिटी'] very बहुत\n",
      "['बड़ा', 'देरी', 'स्पीड', 'जैसा', 'प्रदर्शन'] display प्रदर्शन\n",
      "['समझदारी', 'बदला', 'नहीं', 'कहा', 'पाएगा'] wont नहीं\n",
      "Words added: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877f8ead41d943a892211f1d8a85adde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words matched:- 658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2be68a46feb490bbfc7fc52ff8a680d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words matched:- 667\n",
      "Words added: 0\n"
     ]
    }
   ],
   "source": [
    "# English using Word2Vec\n",
    "save_text = L1\n",
    "while True:\n",
    "    closest = {i:[] for i in save_text.split('\\n')}\n",
    "    n = 5\n",
    "    a = 0\n",
    "    for i in tqdm(save_text.split('\\n')):\n",
    "        try: \n",
    "            closest[i].append([i[0] for i in w2v_eng.most_similar(i.split(',')[0])][:n])\n",
    "            a+=1\n",
    "        except: closest[i].append([None])\n",
    "\n",
    "    print(f\"Number of words matched:- {a}\")\n",
    "\n",
    "    #Hindi using Word2Vec\n",
    "    # closest = {}\n",
    "    n = 5\n",
    "    a = 0\n",
    "    for i in tqdm(save_text.split('\\n')):\n",
    "        try: \n",
    "            closest[i].append([i[0] for i in w2v_hindi.most_similar(i.split(',')[1])][:n])\n",
    "            a+=1\n",
    "        except: closest[i].append([None])\n",
    "    print(f\"Number of words matched:- {a}\")\n",
    "\n",
    "    d = 0\n",
    "    m = ''\n",
    "    dummy_ = [i.split(',')[0] for i in save_text.split('\\n')]\n",
    "    for k,v in closest.items():\n",
    "        if v[0][0] != None and v[1][0] !=None:\n",
    "            for j in v[0]:\n",
    "                if remove_punctuation(j.strip()) in dummy_: continue\n",
    "                try:\n",
    "                    if lex[remove_punctuation(j.strip())] in v[1]:\n",
    "                        m += f\"{remove_punctuation(j.strip())},{lex[remove_punctuation(j.strip())]},{k.split(',')[-1]}\\n\"\n",
    "                        print(v[1],remove_punctuation(j.strip()),lex[remove_punctuation(j.strip())])\n",
    "                        d += 1\n",
    "                except: pass\n",
    "\n",
    "    print(f\"Words added: {d}\")\n",
    "    save_text += m\n",
    "    if d == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"save/w2v.json\", \"w\") as f:\n",
    "    f.write(json.dumps(closest))\n",
    "\n",
    "with open(\"save/w2v.L1.txt\", \"w\") as f:\n",
    "    f.write(save_text)\n",
    "with open(\"save/w2v.json\", \"r\") as f:\n",
    "    out = json.loads(f.read())\n",
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794299c6c2cb4ceab637ca530d1a93f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words matched:- 655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5100e46b12ee4ba3b8c04e63c302d2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2117.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words matched:- 664\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "# v1,v2 = [3, 45, 7, 2], [2, 54, 13, 15]\n",
    "# cosine_similarity(v1,v2)\n",
    "\n",
    "#English using Glove\n",
    "# save_text = L1\n",
    "while True:\n",
    "    closest = {i:[] for i in save_text.split('\\n')}\n",
    "    n = 5\n",
    "    a = 0\n",
    "    for i in tqdm(save_text.split('\\n')):\n",
    "        word = i.split(',')[0]\n",
    "        if word in glove_eng.keys():\n",
    "            dummy = {}\n",
    "            for j in glove_eng.keys():\n",
    "                score = cosine_similarity([float(i) for i in glove_eng[j][-100:]],[float(i) for i in glove_eng[word][-100:]])\n",
    "                dummy[score] = j\n",
    "            closest[i].append([i[-1] for i in sorted(dummy.items(),reverse=True)][1:n+1])\n",
    "            a+=1\n",
    "        else: closest[i].append([None])\n",
    "    print(f\"Number of words matched:- {a}\")\n",
    "\n",
    "    #Hindi using Glove\n",
    "    # closest = {}\n",
    "    n = 5\n",
    "    a = 0\n",
    "    for i in tqdm(save_text.split('\\n')):\n",
    "        try: word = i.split(',')[1]\n",
    "        except: continue\n",
    "        if word in glove_hindi.keys():\n",
    "            dummy = {}\n",
    "            for j in glove_hindi.keys():\n",
    "                score = cosine_similarity([float(i) for i in glove_hindi[j][-100:]],[float(i) for i in glove_hindi[word][-100:]])\n",
    "                dummy[score] = j\n",
    "            closest[i].append([i[-1] for i in sorted(dummy.items(),reverse=True)][1:n+1])\n",
    "            a+=1\n",
    "        else: closest[i].append([None])\n",
    "    print(f\"Number of words matched:- {a}\")\n",
    "\n",
    "    d = 0\n",
    "    m = ''\n",
    "    dummy_ = [i.split(',')[0] for i in save_text.split('\\n')]\n",
    "    for k,v in closest.items():\n",
    "        if v[0][0] != None and v[1][0] !=None:\n",
    "            for j in v[0]:\n",
    "                if remove_punctuation(j.strip()) in dummy_: continue\n",
    "                try:\n",
    "                    if lex[remove_punctuation(j.strip())] in v[1]:\n",
    "                        m += f\"{remove_punctuation(j.strip())},{lex[remove_punctuation(j.strip())]},{k.split(',')[-1]}\\n\"\n",
    "                        print(v[1],remove_punctuation(j.strip()),lex[remove_punctuation(j.strip())])\n",
    "                        d += 1\n",
    "                except: pass\n",
    "\n",
    "    print(f\"Words added: {d}\")\n",
    "    save_text += m\n",
    "    if d == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"save/glove.json\", \"w\") as f:\n",
    "    f.write(json.dumps(closest))\n",
    "\n",
    "with open(\"save/glove.L1.txt\", \"w\") as f:\n",
    "    f.write(save_text)\n",
    "    \n",
    "with open(\"save/glove.json\", \"r\") as f:\n",
    "    out = json.loads(f.read())\n",
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

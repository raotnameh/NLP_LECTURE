{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-2ibXdz8Oy7"
   },
   "source": [
    "# Assignment 3 | Sentiment Analyisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzPLO7KI8LD3"
   },
   "source": [
    "### Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12049,
     "status": "ok",
     "timestamp": 1604509688223,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "80S_izjA6boo",
    "outputId": "2cf0e55c-455a-434b-8242-a5eea147414f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hemant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Requirement already satisfied: tweet-preprocessor in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji in /home/hemant/.local/lib/python3.7/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#installing tweet-preprocessor\n",
    "from tqdm.auto import tqdm\n",
    "!pip install tweet-preprocessor --user\n",
    "import preprocessor as p\n",
    "from pathlib import Path\n",
    "\n",
    "#emoji library\n",
    "!pip install emoji --user\n",
    "import emoji\n",
    "\n",
    "# model save\n",
    "import joblib\n",
    "\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10016,
     "status": "ok",
     "timestamp": 1604509688225,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "2vO89PLYLdSu"
   },
   "outputs": [],
   "source": [
    "class featurise(object):\n",
    "    def __init__(self, x,y):\n",
    "        super(featurise, self).__init__()\n",
    "        self.seq = None\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        ##########################################\n",
    "        self.final_df = pd.DataFrame()\n",
    "        self.final_df['tweets'] = x\n",
    "        self.final_df['target'] = y\n",
    "        ##########################################\n",
    "        self.negation_w = set(['never', 'no', 'nothing', 'nowhere', 'noone', 'none', 'not', 'havent', 'hasnt', 'hadnt', 'cant', \n",
    "                         'couldnt', 'shouldnt', 'wont', 'wouldnt', 'dont', 'doesnt', 'didnt', 'isnt', 'arent', 'aint'] )\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.stop_words.remove('not')\n",
    "\n",
    "    def elongated_word(self, x):\n",
    "        regex_ = re.compile(r\"(.)\\1{2}\")\n",
    "        if regex_.search(x): return True\n",
    "        else: return False\n",
    "\n",
    "    def emoji(self, x): # Doubt\n",
    "        return len([1 for word in regex.findall(r'\\X', x) if any(char in emoji.UNICODE_EMOJI for char in word)])\n",
    "    \n",
    "    def punctuation(self, x):\n",
    "        punct_ = string.punctuation\n",
    "        return len([1 for w in x if w in punct_])\n",
    "    \n",
    "    def all_caps(self, x):\n",
    "        return len([1 for each in x if each.isupper()])\n",
    "\n",
    "    def hashtag_count(self, x):\n",
    "        return len([word for word in x.split() if word.startswith('#') and len(word)>1 ])\n",
    "    \n",
    "    def negation(self, x):\n",
    "        flag = 0\n",
    "        splits = \",.:;!?\"\n",
    "        final = []\n",
    "        count = 0\n",
    "#         x = f\"I don't think I will enjoy it: it might be too spicy.\".lower()\n",
    "        for word in x.split():\n",
    "            if flag ==1:\n",
    "                for symb in splits:\n",
    "                    if symb in word:\n",
    "                        flag = 0\n",
    "                        word_split = word.split(symb)\n",
    "                        \n",
    "                        if len(word_split[0])>0:\n",
    "                        \n",
    "                            count+=1\n",
    "                        \n",
    "                            if len(word_split[-1])>1:\n",
    "                        \n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                if flag==1:\n",
    "                    \n",
    "                    count+=1\n",
    "                \n",
    "            elif flag==0:\n",
    "                \n",
    "                pass\n",
    "\n",
    "            clean = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "            if clean in self.negation_w:\n",
    "                flag=1\n",
    "        return count\n",
    "\n",
    "    def pos_tag_feat(self, a):\n",
    "        tagged_texts = pos_tag_sents(map(word_tokenize, a))\n",
    "\n",
    "        df = pd.DataFrame(a)\n",
    "        df['pos'] = tagged_texts\n",
    "        pos_vocab=set()\n",
    "        for pos in tagged_texts:\n",
    "            for each in pos:\n",
    "                pos_vocab.add(each[1])\n",
    "        def pos_count(pos_sent, pos_vocab):\n",
    "            pos_feature = [0]* len(pos_vocab)\n",
    "            for each in pos_sent:\n",
    "                pos_feature[pos_vocab.index(each[1])] +=1\n",
    "            return pos_feature\n",
    "        pos_feature_df = []\n",
    "        for i in df['pos']:\n",
    "            pos_feature_df.append(pos_count(i,list(pos_vocab)))\n",
    "        df = pd.DataFrame(pos_feature_df, columns=list(pos_vocab))\n",
    "        return df\n",
    "\n",
    "    #########################\n",
    "    \n",
    "    def remove_stopword(self, x):\n",
    "        dummy = []\n",
    "        for i in x.split():\n",
    "            if i not in self.stop_words: dummy.append(i)\n",
    "        return ' '.join(dummy)\n",
    "    \n",
    "    def all_lower(self, x):\n",
    "        return x.lower()\n",
    "    def remove_punctuation(self, x):\n",
    "        \n",
    "        return re.sub(r'[^\\w\\s]', '', x)\n",
    "    def remove_number(self, x):\n",
    "        return ''.join([_ for _ in x if not _.isdigit()])\n",
    "    def rem_un_url(self, x):\n",
    "        return p.clean(x)\n",
    "        \n",
    "    def process(self):\n",
    "        self.final_df['elogated'] = tqdm(self.x.map(self.elongated_word))\n",
    "        self.final_df['emoji'] = tqdm(self.x.map(self.emoji))\n",
    "        self.final_df['punctuation'] = tqdm(self.x.map(self.punctuation))\n",
    "        self.final_df['allcaps'] = tqdm(self.x.map(self.all_caps))\n",
    "        self.final_df['hashtag'] = tqdm(self.x.map(self.hashtag_count))\n",
    "        self.final_df['negation'] = tqdm(self.x.map(self.negation))\n",
    "        \n",
    "        self.final_df['new_tweets'] = tqdm(self.x.map(self.rem_un_url))\n",
    "        self.final_df['new_tweets'] = tqdm(self.final_df['new_tweets'].map(self.all_lower))\n",
    "        self.final_df['new_tweets'] = tqdm(self.final_df['new_tweets'].map(self.remove_number))\n",
    "        \n",
    "        new_df = self.pos_tag_feat(tqdm(list(self.final_df['new_tweets'])))\n",
    "        self.final_df = pd.concat([self.final_df, new_df], axis=1)\n",
    "        \n",
    "        self.final_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "        self.final_df['new_tweets'] = tqdm(self.final_df['new_tweets'].map(self.remove_punctuation))\n",
    "        self.final_df['new_tweets'] = tqdm(self.final_df['new_tweets'].map(self.remove_stopword))\n",
    "        return self\n",
    "\n",
    "################################################################################\n",
    "def printdf(df, rows = 5):\n",
    "    print(df[:rows].to_markdown())\n",
    "    return True \n",
    "  \n",
    "#unigrams bow\n",
    "def bow(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "\n",
    "    count_vect = CountVectorizer() #in scikit-learn\n",
    "    count_vect.fit(X_train)\n",
    "\n",
    "    bow_counts_train = count_vect.transform(X_train)\n",
    "    bow_counts_cv = count_vect.transform(X_cv)\n",
    "    bow_counts_test = count_vect.transform(X_test)\n",
    "    print(\"the shape of out text BOW vectorizer \",bow_counts_train.get_shape())\n",
    "    print(\"the number of unique words \", bow_counts_train.get_shape()[1])\n",
    "    # final_counts_test=final_counts_test.toarray()\n",
    "    return bow_counts_train, bow_counts_cv, bow_counts_test\n",
    "\n",
    "#bigrams bow\n",
    "def bigram(X_train, X_test, X_cv, y_train, y_test, y_cv):\n",
    "    count_vect = CountVectorizer(ngram_range=(2,2))\n",
    "    bigram_counts_train = count_vect.fit_transform(X_train)\n",
    "    bigram_counts_cv = count_vect.transform(X_cv)\n",
    "    bigram_counts_test = count_vect.transform(X_test)\n",
    "    print(\"some feature names \", count_vect.get_feature_names()[:10])\n",
    "    print(\"the shape of out  BIGRAM vectorizer \",bigram_counts_train.get_shape())\n",
    "    print(\"the number of unique words with bigrams \", bigram_counts_train.get_shape()[1])\n",
    "    return bigram_counts_train, bigram_counts_cv, bigram_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6820,
     "status": "ok",
     "timestamp": 1604509829782,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "FtGOG6pM7nKC",
    "outputId": "28df25b8-3d3d-4409-b6e4-745416caf55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120000, 6)\n",
      "(480000, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/3a/sentiment_train.csv\")\n",
    "test = pd.read_csv(\"data/3a/sentiment_test.csv\")\n",
    "\n",
    "# uncomment this later \n",
    "# train = train[:300]\n",
    "# test = test[:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27228,
     "status": "ok",
     "timestamp": 1604509710650,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "Qwf3CBZ223dJ",
    "outputId": "2479cf50-9ab4-4b3c-8cfc-e14b087555ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Found\n",
      "CSV Found\n",
      "CSV Found\n"
     ]
    }
   ],
   "source": [
    "X_train = train[\"5\"]\n",
    "y_train = train[\"0\"]\n",
    "X_test = test[\"5\"]\n",
    "y_test = test['0']\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#### FEATURISATION\n",
    "\n",
    "my_file = Path(\"data/3a/train_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    train_df = pd.read_csv(\"data/3a/train_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise train data\n",
    "    proc = featurise(X_train,y_train).process()\n",
    "    train_df = proc.final_df\n",
    "    train_df.to_csv('data/3a/train_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/val_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    val_df = pd.read_csv(\"data/3a/val_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise cv data\n",
    "    proc = featurise(X_cv,y_cv).process()\n",
    "    val_df = proc.final_df\n",
    "    val_df.to_csv('data/3a/val_df.csv')\n",
    "\n",
    "my_file = Path(\"data/3a/test_df.csv\")\n",
    "if my_file.is_file():\n",
    "    print(\"CSV Found\")\n",
    "    test_df = pd.read_csv(\"data/3a/test_df.csv\")\n",
    "else:\n",
    "    print(\"Generating CSV\")\n",
    "    #featurise test data\n",
    "    proc = featurise(X_test,y_test).process()\n",
    "    test_df = proc.final_df\n",
    "    test_df.to_csv('data/3a/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 537824,
     "status": "ok",
     "timestamp": 1604498381196,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "sFYM9N42oFnK"
   },
   "outputs": [],
   "source": [
    "# train_df.isnull().values.any()\n",
    "# train_df['new_tweets'].isnull().sum()\n",
    "# df['your column name'].isnull().sum()\n",
    "train_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "val_df.dropna(subset = [\"new_tweets\"], inplace=True)\n",
    "test_df.dropna(subset = [\"new_tweets\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593820,
     "status": "ok",
     "timestamp": 1604498441480,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "d8kHJBcC34W_",
    "outputId": "298c059a-e285-4272-9e4e-36d1ee8c0744"
   },
   "outputs": [],
   "source": [
    "bow_counts_train, bow_counts_cv, bow_counts_test = bow(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])\n",
    "bigram_counts_train, bigram_counts_cv, bigram_counts_test = bigram(train_df['new_tweets'], test_df['new_tweets'], val_df['new_tweets'], train_df['target'], test_df['target'], val_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 610735,
     "status": "ok",
     "timestamp": 1604498462093,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "1u2OIERBS07X"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def save_npz(f,name):\n",
    "    sparse.save_npz(\"data/3a/npz/\"+str(name)+\".npz\", f)\n",
    "    return True\n",
    "\n",
    "save_npz(bow_counts_cv, \"bow_counts_cv\")\n",
    "save_npz(bow_counts_test, \"bow_counts_test\")\n",
    "save_npz(bow_counts_train, \"bow_counts_train\")\n",
    "save_npz(bigram_counts_train, \"bigram_counts_train\")\n",
    "save_npz(bigram_counts_test, \"bigram_counts_test\")\n",
    "save_npz(bigram_counts_cv, \"bigram_counts_cv\")\n",
    "\n",
    "# train_df.to_csv('data/3a/gram_traincsv.csv')\n",
    "# val_df.to_csv('data/3a/gram_valcsv.csv')\n",
    "# test_df.to_csv('data/3a/gram_testcsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1604491907199,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "XEkHXz3XCCMb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1604493558672,
     "user": {
      "displayName": "ASEEM SRIVASTAVA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN5a5KRj_7RvphfJgV8OFNXiwT_pGHrCsvPErLaA=s64",
      "userId": "12200875993004013816"
     },
     "user_tz": -330
    },
    "id": "rJvJUSIwJDmX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2FMJ3VHPU0ttKEQbYUYon",
   "collapsed_sections": [],
   "mount_file_id": "1rSUMnud92JcJuJyZ5Rh2ITMs9pKt7nE3",
   "name": "3a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

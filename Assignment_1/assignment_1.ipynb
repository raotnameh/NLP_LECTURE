{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignmet 1 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from word2number import w2n\n",
    "\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "consonants = set(' '.join(string.ascii_lowercase).split()) - set(vowels)\n",
    "punctutaions= set(' '. join(string.punctuation).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txt_data():\n",
    "    '''\n",
    "    Master class to load and return the data into sentences, tokens and words\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,path,punctutaions):\n",
    "        self.path = path\n",
    "        self.txt = self.open_()\n",
    "        \n",
    "        paras = self.txt.split(\"\\n\\n\")\n",
    "        sents = [self.para_to_sent(j) for j in paras]\n",
    "        self.sents = [k.replace(\"\\n\", ' ') for j in sents for k in j]\n",
    "        self.tokens = [k for m in [self.sent_to_tokens(k) for k in self.sents] for k in m]\n",
    "        self.words = self.words_(self.tokens,punctutaions)\n",
    "        \n",
    "    def open_(self):\n",
    "        '''\n",
    "        To read a text file\n",
    "        path: path to the text file\n",
    "        '''\n",
    "        with open(self.path, \"rb\") as f:\n",
    "            try: return f.read().decode(\"utf8\")\n",
    "            except: return f.read()\n",
    "            \n",
    "    def para_to_sent(self,txt):\n",
    "        '''\n",
    "        Task: 1_1\n",
    "        converts an input txt file to sentences\n",
    "        txt = input txt file in string format\n",
    "        '''\n",
    "        return sent_tokenize(re.sub(' +', ' ',txt.strip()))\n",
    "\n",
    "    def sent_to_tokens(self,txt):\n",
    "        '''\n",
    "        converts an input sentence to words\n",
    "        txt = input txt file in string format\n",
    "        '''\n",
    "        return word_tokenize(txt)\n",
    "\n",
    "    def words_(self,tokens, punctutaions):\n",
    "        '''\n",
    "        Task 1_2\n",
    "        Convert token to words\n",
    "        tokens: Output of sent_to_tokens func\n",
    "        punctuations: punctuations need to be removed from the tokens\n",
    "        '''\n",
    "        return [i for i in tokens if i not in punctutaions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "path = 'dev.txt' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "\n",
    "output = f\"Assignment 1 output.pdf \\nFile used is: {path} \\n\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: \n",
      "             Number of sentences present are: 28 \n",
      "             Number of tokens present are: 604 \n",
      "             Number of words present are: 471 \n",
      "             Number of unique words/tokens are: 268/286.\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "sents = input_data.sents\n",
    "tokens = input_data.tokens\n",
    "words = input_data.words\n",
    "\n",
    "print(f\"Task 1: \\n \\\n",
    "            Number of sentences present are: {len(sents)} \\n \\\n",
    "            Number of tokens present are: {len(tokens)} \\n \\\n",
    "            Number of words present are: {len(words)} \\n \\\n",
    "            Number of unique words/tokens are: {len(set(words))}/{len(set(tokens))}.\")\n",
    "\n",
    "output+= f\"Task 1: \\n \\\n",
    "            Number of sentences present are: {len(sents)} \\n \\\n",
    "            Number of tokens present are: {len(tokens)} \\n \\\n",
    "            Number of words present are: {len(words)} \\n \\\n",
    "            Number of unique words/tokens are: {len(set(words))}/{len(set(tokens))}.\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: \n",
      "             Number of words starting with consonants are: 286 \n",
      "             Number of words starting with vowels are: 98 \n",
      "             Number of unique words starting with consonants/vowels are: 156/52. \n"
     ]
    }
   ],
   "source": [
    "#Task 2\n",
    "\n",
    "def num_2_c(words):\n",
    "    '''\n",
    "    Task 2_1\n",
    "    return number of words starting with consonants\n",
    "    words: list of words\n",
    "    '''\n",
    "    return [i for i in words if i[0] in consonants]\n",
    "def num_2_v(words):\n",
    "    '''\n",
    "    Task 2_2\n",
    "    return number of words starting with vowels\n",
    "    words: list of words\n",
    "    '''\n",
    "    return [i for i in words if i[0] in vowels]\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "words = input_data.words\n",
    "\n",
    "print(f\"Task 2: \\n \\\n",
    "            Number of words starting with consonants are: {len(num_2_c(words))} \\n \\\n",
    "            Number of words starting with vowels are: {len(num_2_v(words))} \\n \\\n",
    "            Number of unique words starting with consonants/vowels are: {len(set(num_2_c(words)))}/{len(set(num_2_v(words)))}. \")\n",
    "\n",
    "output+= f\"Task 2: \\n \\\n",
    "            Number of words starting with consonants are: {len(num_2_c(words))} \\n \\\n",
    "            Number of words starting with vowels are: {len(num_2_v(words))} \\n \\\n",
    "            Number of unique words starting with consonants/vowels are: {len(set(num_2_c(words)))}/{len(set(num_2_v(words)))}. \\n\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: \n",
      "         Number of email IDs present in the given text file are: 7 \n",
      "         Number of UNIQUE email IDs present in the given text file are: 5 \n",
      "         And the email ID are as follows: \n",
      "\t\trobg@apple.com\n",
      "\t\tnews@gallant.apple.com\n",
      "\t\trobg-230493103008@sideout.apple.com\n",
      "\t\t1r941o$3tu@menudo.uh.edu\n",
      "\t\t1r941o$3tu@menudo.uh.edu\n",
      "\t\tinde7wv@Rosie.UH.EDU\n",
      "\t\trobg@apple.com\n"
     ]
    }
   ],
   "source": [
    "#Task 3\n",
    "\n",
    "def find_email(sents):\n",
    "    '''\n",
    "    Task 3\n",
    "    sents: A list containing the txt\n",
    "    returns a list of all the email id's present in the list given\n",
    "            the e\n",
    "            mail ids has a \".\" in it.\n",
    "    '''\n",
    "    dummy = []\n",
    "    for i in sents:\n",
    "        if re.findall(r\"\\b\\S+@\\S+\\b\",i):\n",
    "            dummy.append(re.findall(r\"\\b\\S+@\\S+\\.+\\S+\\b\",i))\n",
    "    return [i for j in dummy for i in j]\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "sents = input_data.sents\n",
    "\n",
    "print(f\"Task 3: \\n \\\n",
    "        Number of email IDs present in the given text file are: {len(find_email(sents))} \\n \\\n",
    "        Number of UNIQUE email IDs present in the given text file are: {len(set(find_email(sents)))} \\n \\\n",
    "        And the email ID are as follows: \")\n",
    "\n",
    "output+= f\"Task 3: \\n \\\n",
    "        Number of email IDs present in the given text file are: {len(find_email(sents))} \\n \\\n",
    "        Number of UNIQUE email IDs present in the given text file are: {len(set(find_email(sents)))} \\n \\\n",
    "        And the email ID are as follows: \\n\"\n",
    "\n",
    "for i in find_email(sents):\n",
    "    print(f\"\\t\\t{i}\")\n",
    "    output+= f\"\\t\\t{i} \\n\"\n",
    "    \n",
    "output+= \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task 4: \n",
      "             Number of sentences starting with --\"last\"-- are: 0 \n",
      "             And the sentences are as follows: \n"
     ]
    }
   ],
   "source": [
    "#Task 4\n",
    "\n",
    "def task_4(sents,given_word): # case insensitive\n",
    "    '''\n",
    "    Task 4\n",
    "    returns a list of sentences starting with a given word\n",
    "    sesnts: list of seneteces\n",
    "    given_words: word to look for\n",
    "    '''\n",
    "    dummy = []\n",
    "    for _i in sents:        \n",
    "        if re.findall(fr\"^{given_word} \\b\",_i.lower()):\n",
    "            dummy.append(_i)\n",
    "    return dummy\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "sents = input_data.sents\n",
    "\n",
    "word = 'last' # Enter given word to search for in a document\n",
    "try : word = w2n.word_to_num(word)\n",
    "except: word = word.lower()\n",
    "\n",
    "print(f\" Task 4: \\n \\\n",
    "            Number of sentences starting with --\\\"{word}\\\"-- are: {len(task_4(sents,word))} \\n \\\n",
    "            And the sentences are as follows: \")\n",
    "\n",
    "output += f\" Task 4: \\n \\\n",
    "            Number of sentences starting with --\\\"{word}\\\"-- are: {len(task_4(sents,word))} \\n \\\n",
    "            And the sentences are as follows: \\n\"\n",
    "for i in task_4(sents,word):\n",
    "    print(f\"\\t\\t{i}\")\n",
    "    output+= f\"\\t\\t{i}\\n\"\n",
    "    \n",
    "output+= \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task 5: \n",
      "             Number of sentences ending with --\"you\"-- are: 1 \n",
      "             And the sentences are as follows: \n",
      "\t\tAny other ideas for being first in line with no traffic directly behind you?\n"
     ]
    }
   ],
   "source": [
    "#Task 5\n",
    "\n",
    "def task_5(sents,given_word): # case insensitive\n",
    "    '''\n",
    "    Task 5\n",
    "    returns a list of sentences ending with a given word\n",
    "    sesnts: list of seneteces\n",
    "    given_word: word to look for\n",
    "    \n",
    "    Assumption: -you- and -you.- are considered same.\n",
    "                other punctuaions are ----.?!\"\"''----\n",
    "    '''\n",
    "    dummy = []\n",
    "    for _i in sents:\n",
    "        if re.findall(fr\"\\b {given_word}[.?,!\"\"'']*$\",_i.lower()):\n",
    "            dummy.append(_i)\n",
    "    return dummy\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "sents = input_data.sents\n",
    "word = 'you' # Enter given word to search for in a document\n",
    "try : word = w2n.word_to_num(word)\n",
    "except: word = word.lower()\n",
    "print(f\" Task 5: \\n \\\n",
    "            Number of sentences ending with --\\\"{word}\\\"-- are: {len(task_5(sents,word))} \\n \\\n",
    "            And the sentences are as follows: \")\n",
    "\n",
    "output+= f\" Task 5: \\n \\\n",
    "            Number of sentences ending with --\\\"{word}\\\"-- are: {len(task_5(sents,word))} \\n \\\n",
    "            And the sentences are as follows: \\n\"\n",
    "for i in task_5(sents,word):\n",
    "    print(f\"\\t\\t{i}\")\n",
    "    output+= f\"\\t\\t{i}\\n\"\n",
    "    \n",
    "output+= \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task 5: \n",
      "             Number of sentences containing the word--\"50\"-- are: 1 \n",
      "             Number of words containing the word--\"50\"-- are: 1 \n",
      "             And the sentences are as follows: \n",
      "\t\t- If you are the last in a line of 50 vehicles, watch your mirrors  *constantly!\n"
     ]
    }
   ],
   "source": [
    "#Task 6\n",
    "\n",
    "def task_6(words,sents,given_word): # case insensitive\n",
    "    '''\n",
    "    Task 6\n",
    "    return a list of all the sentences and words given a word to look for \n",
    "    words = list of words from in the document\n",
    "    sents = list of sentences in the document\n",
    "    given_word = word to look for\n",
    "    '''\n",
    "    sents_ = [i for i in sents if re.findall(fr\"\\b{given_word}\\b\",i.lower())]\n",
    "    words_ = [i for i in words if re.findall(fr\"\\b{given_word}\\b\",i.lower())]\n",
    "    return sents_, words_\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "words = input_data.words\n",
    "sents = input_data.sents\n",
    "word = \"50\" # Enter given word to search for in a document\n",
    "try : word = w2n.word_to_num(word)\n",
    "except: word = word.lower()\n",
    "num_sents, num_words = task_6(words, sents, word)\n",
    "print(f\" Task 5: \\n \\\n",
    "            Number of sentences containing the word--\\\"{word}\\\"-- are: {len(num_sents)} \\n \\\n",
    "            Number of words containing the word--\\\"{word}\\\"-- are: {len(num_words)} \\n \\\n",
    "            And the sentences are as follows: \")\n",
    "\n",
    "output+= f\" Task 5: \\n \\\n",
    "            Number of sentences containing the word--\\\"{word}\\\"-- are: {len(num_sents)} \\n \\\n",
    "            Number of words containing the word--\\\"{word}\\\"-- are: {len(num_words)} \\n \\\n",
    "            And the sentences are as follows: \\n\"\n",
    "for i in num_sents:\n",
    "    print(f\"\\t\\t{i}\")\n",
    "    output+= f\"\\t\\t{i}\\n\"\n",
    "    \n",
    "output+= \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 7: \n",
      "             Number of questions present are: 0 \n",
      "             And the questions are as follows: \n"
     ]
    }
   ],
   "source": [
    "#Task 7\n",
    "\n",
    "def task_7(sents): \n",
    "    '''\n",
    "    Task 7\n",
    "    Return a list of questions present in the given sentences\n",
    "    sents = list of sentences\n",
    "    '''\n",
    "    return [i for i in sents if re.findall(r\"\\?+$\\b\",i)]\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "sents = input_data.sents\n",
    "print(f\"Task 7: \\n \\\n",
    "            Number of questions present are: {len(task_7(sents))} \\n \\\n",
    "            And the questions are as follows: \")\n",
    "\n",
    "output+= f\"Task 7: \\n \\\n",
    "            Number of questions present are: {len(task_7(sents))} \\n \\\n",
    "            And the questions are as follows: \\n\"\n",
    "\n",
    "for i in task_7(sents):\n",
    "    print(f\"\\t\\t--{i}\")\n",
    "    output+= f\"\\t\\t--{i}\\n\"\n",
    "    \n",
    "output+= \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 8: \n",
      "         Mins and Secs present in the file are as follows: \n",
      "\t\t--41 mins, 48 sec\n"
     ]
    }
   ],
   "source": [
    "#Task 8\n",
    "\n",
    "def task_8(para):\n",
    "    '''\n",
    "    Task 8\n",
    "    return the time stamp present in a document\n",
    "    para: text file as an input\n",
    "    '''\n",
    "    dummy = []\n",
    "    for i in re.findall(r\"[0-9]+[0-9]+:[0-9]+[0-9]+:[0-9]+[0-9]+\", para):\n",
    "        time = i.split(\":\")\n",
    "        dummy.append(time[-2] + ' mins, ' +time[-1] + ' sec')\n",
    "    return dummy\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "txt = input_data.txt\n",
    "print(f\"Task 8: \\n \\\n",
    "        Mins and Secs present in the file are as follows: \")\n",
    "\n",
    "output+= f\"Task 8: \\n \\\n",
    "        Mins and Secs present in the file are as follows: \\n\"\n",
    "for i in task_8(txt):\n",
    "    print(f\"\\t\\t--{i}\")\n",
    "    output+= f\"\\t\\t--{i}\\n\"\n",
    "    \n",
    "output+= \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 9: \n",
      "         Number o`f abbreviations present in the file are: 8 \n",
      "         Number of UNIQUE abbreviations present in the file are: 8 \n",
      "         Abbreviations present in the file are as follows: \n",
      "\t\t--Message-ID\n",
      "\t\t--GMT\n",
      "\t\t--Rosie.UH.EDU\n",
      "\t\t--BDC\n",
      "\t\t--XYZ\n",
      "\t\t--NOTE\n",
      "\t\t--Erasure.ShonaLaing.WildSwans.B52s.U2.JudyBats.REM.Smiths\n",
      "\t\t--FAA\n"
     ]
    }
   ],
   "source": [
    "#Task 9\n",
    "\n",
    "def task_9(words):\n",
    "    '''\n",
    "    Task 9\n",
    "    returns the Abbreviations present in the file\n",
    "    para = list of words present in the file\n",
    "    '''\n",
    "    return [j for j in words if re.findall(fr\"\\b[0-9]*[A-Z]+[0-9]*[A-Z]+[0-9]*\\b\",j)]\n",
    "\n",
    "\n",
    "# path = 'data/20_newsgroups/rec.motorcycles/103117' # path to the input file\n",
    "input_data = txt_data(path, punctutaions)\n",
    "words = input_data.words\n",
    "\n",
    "print(f\"Task 9: \\n \\\n",
    "        Number o`f abbreviations present in the file are: {len(task_9(words))} \\n \\\n",
    "        Number of UNIQUE abbreviations present in the file are: {len(set(task_9(words)))} \\n \\\n",
    "        Abbreviations present in the file are as follows: \")\n",
    "\n",
    "output+= f\"Task 9: \\n \\\n",
    "        Number of abbreviations present in the file are: {len(task_9(words))} \\n \\\n",
    "        Number of UNIQUE abbreviations present in the file are: {len(set(task_9(words)))} \\n \\\n",
    "        Abbreviations present in the file are as follows: \\n\"\n",
    "\n",
    "for i in task_9(words):\n",
    "    print(f\"\\t\\t--{i}\")\n",
    "    output+= f\"\\t\\t--{i}\\n\"\n",
    "output+= \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\",\"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

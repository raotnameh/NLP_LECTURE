{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assignment_2.ipynb  brown.txt  exp_results.json  question.pdf\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random, itertools\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txt_data():\n",
    "    '''\n",
    "    Master class to load  the dataset\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,path,split=0.9,shuffle=False):\n",
    "        '''\n",
    "        path: path to dataset\n",
    "        split: train/val split\n",
    "        shuffle: shuffle the dataset randomly before splitting or no.\n",
    "        '''\n",
    "        self.path = path\n",
    "        \n",
    "        # list of sentences of list of tuples defining pos_tag and words.\n",
    "        self.data = self.___sent_to_tags___(self.___open___(self)) \n",
    "        if shuffle: random.shuffle(self.data)\n",
    "        self.train =  self.data[:int(len(self.data)*split)]\n",
    "        self.val = self.data[int(len(self.data)*split):]\n",
    "        \n",
    "        self.pos_tags = [i for i in set([j[0] for i in self.data for j in i])] #list of unique pos_tags\n",
    "        self.vocab = [i for i in set([j[1] for i in self.data for j in i])] # list of vocab\n",
    "            \n",
    "    \n",
    "    def ___open___(self,path):\n",
    "        '''\n",
    "        To read a text file\n",
    "        path: path to the text file\n",
    "        '''\n",
    "        with open(self.path, \"r\") as f:\n",
    "            return f.readlines()\n",
    "    \n",
    "    def ___sent_to_tags___(self,sents):\n",
    "        '''\n",
    "        converts a raw sentence to tags and words as a tuple\n",
    "        '''\n",
    "        dummy = []\n",
    "        for i in tqdm(sents):\n",
    "            d = []\n",
    "            for j in i.split():\n",
    "                try: d.append((j.split('_')[1],j.split('_')[0].lower()))\n",
    "                except: pass #print(f\"--{i}--{j}\\n\")\n",
    "            dummy.append(d)\n",
    "        return dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converitng emission/transition matrix to prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(matrix): \n",
    "    '''\n",
    "    Converitng emission/transition matrix to prob\n",
    "    matrix: list of lists format\n",
    "    \n",
    "    return: prob (2D numpy array)\n",
    "    '''\n",
    "    dummy = []\n",
    "    for i in np.array(matrix):\n",
    "        if np.sum(i) !=0: dummy.append(i/np.sum(i))\n",
    "        else: dummy.append(i)\n",
    "    return np.array(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(matrix, name='simple'):\n",
    "    '''\n",
    "    Smoothing operation to handle zero values\n",
    "    matrix: 2D numpy array\n",
    "    name: name of smoothing technique\n",
    "    \n",
    "    return: smoothed prob (numpy array)\n",
    "    '''\n",
    "    if name == 'simple':\n",
    "        matrix[matrix == 0] = 0.000000001*matrix.mean()\n",
    "        return matrix\n",
    "    else: raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emission matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_matrix(data,vocab,pos_tags,smooth='simple'):\n",
    "    '''\n",
    "    emission matrix: count of words given the pos_tag\n",
    "    \n",
    "    data: list of sents with the tags: format is list of list of sentences with tag and word as a tuple\n",
    "    vocab: list of vocab\n",
    "    pos_tags: list of pos_tags\n",
    "    smooth: name of smooting technique to use\n",
    "    \n",
    "    return list of pos_tags and a list of matrix (pos_tags,vocab) with count(vocab/pos_tags)\n",
    "    '''\n",
    "    data = [i for j in data for i in j] # list of pos_tag and word pair: [('VB', 'suppose'), ('NP', 'lauren'),()]\n",
    "    dummy = {s[0]:[] for s in data} # dictioanry with tag and list of words as key, value pair, {'AT' :['the', 'the', 'a', 'the'],}\n",
    "    for i in data:\n",
    "        dummy[i[0]].append(i[1])\n",
    "        \n",
    "    e_matrix = [] # emission count matrix with tag and count of words as row, column pair.\n",
    "    for i in tqdm(pos_tags):\n",
    "        if i in dummy.keys():\n",
    "            count = Counter(dummy[i]) # count of words in a list. \n",
    "            e_matrix.append([count[j] for j in vocab]) # vocab(50k) loop takes time not the man loop\n",
    "        else: e_matrix.append([0 for j in vocab])\n",
    "    \n",
    "    return pos_tags, smoothing(prob(e_matrix),name='simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def t_matrix(data, pos_tags, n_gram=2, smooth='simple'):\n",
    "    '''\n",
    "    transition matrix: count of current_pos-tag given the last_pos-tag\n",
    "    \n",
    "    data = list of sents with the tags: format is list of list of sentences with tag and word as a tuple\n",
    "    pos_tags = list of pos_tags\n",
    "    n_gram = look back + 1 i.e, number of words tollok back to calculate the transition matrix\n",
    "    smooth: name of smooting technique to use\n",
    "    \n",
    "    return a list of matrix (pos_tags**n_gram,pos_tags) with count(pos_tags/pos_tags**_gram)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data = [[j[0]  for j in i] for i in data] # list of tags for each sentence: [['AT', 'NN', CS', '.'],[],] \n",
    "    #list of n_grams of tags, for 2 grams [['AT', 'NN'], ['NN', 'MD'], ['MD', 'HV'],[],]\n",
    "    tags = [i[j:j+n_gram] for i in data for j in range(len(i) - n_gram + 1)]\n",
    "    print(f\" Number of {n_gram} grams present in the dataset: {len(tags)/1000000} million\")\n",
    "    # list of tags of cartesian product all the possible combination of pos_tag pair\n",
    "    keys = [' '.join([j for j in i]) for i in list(itertools.product(pos_tags, repeat=n_gram-1))]\n",
    "    keys.append(\"<s>\") # start symbol <s>\n",
    "     \n",
    "    tags_ = {i:[] for i in keys} # initialize an empty dictionary, dict with\n",
    "    for i in tags:\n",
    "        a = ''\n",
    "        for j in i[:-1]:\n",
    "            a+= f\"{j} \"\n",
    "        tags_[a.strip()].append(i[-1])\n",
    "    tags_['<s>'] = [j for i in data for j in i] # list of all the tags in the dataset to count the start_prob\n",
    "    \n",
    "    t_matrix = [] # transition count matrix with n_gram tag and count of pos_tags as key, value pair.\n",
    "    for i in tqdm(keys):\n",
    "        count = Counter(tags_[i])\n",
    "        d = [count[j] for j in pos_tags]\n",
    "        t_matrix.append(d)\n",
    "\n",
    "    return keys[:-1], smoothing(prob(t_matrix[:-1]),name='simple'), smoothing(prob([t_matrix[-1]])[0],name='simple') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(obs, pos_tags, t_tag, s_prob, t_prob, e_prob, n_gram, t_2=None):\n",
    "    '''\n",
    "    Viterbi decoding: \"https://en.wikipedia.org/wiki/Viterbi_algorithm\"\n",
    "    \n",
    "    obs: list of words in the sentence\n",
    "    pos_tags: list of pos_tags\n",
    "    s_prob: starting probabilty\n",
    "    t_prob: transition probability\n",
    "    e_prob: emission probability\n",
    "    n_gram: number of words to look back +1\n",
    "    \n",
    "    return: top path\n",
    "    '''\n",
    "    path = { s:[] for s in pos_tags} # list of all the previous path pos_tags took\n",
    "    curr_prob = {s:s_prob[s]*e_prob[s][obs[0]] for s in pos_tags} # first word/time-step prob\n",
    "\n",
    "    for i in range(1, len(obs)):\n",
    "        prev_prob = curr_prob\n",
    "        curr_prob = {}\n",
    "        for curr_state in pos_tags:\n",
    "            if i > n_gram - 1 and n_gram > 2: # for the n_gram case\n",
    "                max_prob, state = max(((prev_prob[last_state]*t_prob[f\"{path[curr_state][-1:][0]} {last_state}\"][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            elif n_gram > 2: # simple look back at the last word \n",
    "                max_prob, state = max(((prev_prob[last_state]*t_2[last_state][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            else:\n",
    "                max_prob, state = max(((prev_prob[last_state]*t_prob[last_state][curr_state]*e_prob[curr_state][obs[i]], last_state) \n",
    "                                           for last_state in pos_tags))\n",
    "            curr_prob[curr_state] = max_prob\n",
    "            path[curr_state].append(state)\n",
    "\n",
    "    # find the final largest probability\n",
    "    max_prob = -1\n",
    "    max_path = None\n",
    "    for l, p in path.items():\n",
    "        p.append(l)\n",
    "        if curr_prob[l] > max_prob:\n",
    "            max_path = p\n",
    "            max_prob = curr_prob[l]\n",
    "            \n",
    "    return max_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31c116cd01f4bb9a679f0dec8e4e402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=55145.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"brown.txt\"\n",
    "brown = txt_data(path)\n",
    "train, val = brown.train, brown.val\n",
    "# pos_tags, vocab = brown.pos_tags, brown.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ebf18931fb4cb8a5f8229affa56eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "folds = {} # stores the data of train and test for n_split folds\n",
    "for r, index in tqdm(enumerate(cv.split(brown.data)), total = n_splits):\n",
    "    train = [j for k,j in enumerate(brown.data) if k in index[0]]\n",
    "    test = [j for k,j in enumerate(brown.data) if k in index[1]]\n",
    "    folds[f\"fold_{r}\"]= {\n",
    "                            'train': train,\n",
    "                            'test': test,\n",
    "                            'pos_tags': brown.pos_tags,\n",
    "                            'vocab': brown.vocab\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallel code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel code implementation\n",
    "def run(decoding, txt):\n",
    "    with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "        results = list( tqdm(executor.map(decoding, txt), total=len(txt)) )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds-/-n_gram used: 3-/-2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba80726a4d2491380e0e6dd39ce5791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------: fold_0 \n",
      "\n",
      "length of train and test data : 36763/18382\n",
      "Creating the emission and transition probability matrix: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d3ff800228465bbd1c0f49ea17c698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished emission prob matrix\n",
      " Number of 2 grams present in the dataset: 0.738769 million\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342d14d076914cefa586863417ed571b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished tansition prob matrix\n",
      "Creating emission and transition probability dict for O(1) searching\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1fd79cad9d4e909eb2e04d6da49451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e42bfb6c9944b398be27e915f4bf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_gram = 2 # markov assumption length: if it's 2 we need trigrams, thus value would be 3 (assumption length + 1) \n",
    "exp_results = {} # saving the results in the dict\n",
    "\n",
    "print(f\"Number of folds-/-n_gram used: {len(folds)}-/-{n_gram}\\n\\n\")\n",
    "for fold, data in tqdm(folds.items()):\n",
    "    save_dict = {}\n",
    "    print(f\"\\n\\n{'-'*15}: {fold} \\n\\nlength of train and test data : {len(data['train'])}/{len(data['test'])}\")\n",
    "    \n",
    "    save_dict['n_gram'] =  n_gram\n",
    "    \n",
    "    train = data['train']\n",
    "    val = data['test']\n",
    "    pos_tags = data['pos_tags']\n",
    "    vocab = data['vocab']\n",
    "\n",
    "    print(f\"Creating the emission and transition probability matrix: \")\n",
    "    e_tag, e_prob = e_matrix(train, vocab, pos_tags) # emission prob and pos_tags\n",
    "    print(f\"Finished emission prob matrix\")\n",
    "    # transition prob and tags\n",
    "    if n_gram == 3: # if n_gram is 3 we also need 2_gram prob for the first index\n",
    "        t_tag, t_prob_2, start_probability = t_matrix(train, pos_tags, 2) \n",
    "        t_tag, t_prob, start_probability = t_matrix(train, pos_tags, n_gram) \n",
    "    if n_gram == 2: t_tag, t_prob, start_probability = t_matrix(train, pos_tags, n_gram) \n",
    "    print(f\"Finished tansition prob matrix\")\n",
    "    \n",
    "    print(f\"Creating emission and transition probability dict for O(1) searching\")\n",
    "    # creating emission and transition probability dict for O(1) searching\n",
    "    if n_gram == 3: t_2 = {i: {pos_tags[f]:j for f,j in enumerate(t_prob_2[r])} for r,i in tqdm(enumerate(pos_tags), total=len(pos_tags))}\n",
    "    else: t_2 = None\n",
    "    transition_probability = {i: {pos_tags[f]:j for f,j in enumerate(t_prob[r])} for r,i in tqdm(enumerate(t_tag), total=len(t_tag))}\n",
    "    emission_probability = {i: {vocab[f]:j for f,j in enumerate(e_prob[r])} for r,i in tqdm(enumerate(e_tag), total=len(e_tag))}\n",
    "    start_prob = {i:start_probability[r] for r,i in enumerate(pos_tags)} \n",
    "    # start_prob = {i:1.0 for r,i in enumerate(t_tag)} # if wanted the same starting prob for all pos_tags\n",
    "    \n",
    "    print(f\"Starting viterbi decoding: \")\n",
    "    out = []\n",
    "    true = []\n",
    "#     print(f\"Length of validation set is: {len(val)}\")\n",
    "#     for i in tqdm(val):\n",
    "#         true.append(i)\n",
    "#         obs = [j[1] for j in i]\n",
    "#         out.append(decoding(obs, pos_tags, t_tag, start_prob, transition_probability, emission_probability, n_gram, t_2))\n",
    "    \n",
    "    txt = []\n",
    "    for i in tqdm(val[:10]):\n",
    "        true.append(i)\n",
    "        obs = [j[1] for j in i]\n",
    "        txt.append([obs, pos_tags, t_tag, start_prob, transition_probability, emission_probability, n_gram, t_2])\n",
    "    \n",
    "    print(\"starting the processes\")\n",
    "    temp = run(decoding, txt)\n",
    "    print(\"saving the files\")\n",
    "    \n",
    "    \n",
    "    save_dict['true_val'] = true\n",
    "    save_dict['pred_val'] = out\n",
    "    \n",
    "    print(f\"starting metric calcualtion\")\n",
    "    true = [i[0] for j in true for i in j]\n",
    "    out = [j for i in out for j in i]\n",
    "    assert len(true) == len(out)\n",
    "\n",
    "    \n",
    "    #pos to index and index to pos dict for easy searching\n",
    "    pos_index = {i:e for e,i in enumerate(pos_tags)}\n",
    "    index_pos = {e:i for e,i in enumerate(pos_tags)}\n",
    "    \n",
    "    save_dict['pos_index'] = pos_index\n",
    "    \n",
    "    # adding the true and predictions in a single list\n",
    "    expected = [pos_index[i] for i in  true]\n",
    "    predicted = [pos_index[i] for i in  out]\n",
    "    \n",
    "    labels = sorted(pos_index.values())\n",
    "    \n",
    "    eps = 0\n",
    "\n",
    "    dummy = OrderedDict()\n",
    "    for i in labels:\n",
    "        dummy[str(i)] = {str(j) : eps for j in labels}\n",
    "    for r, i in enumerate(expected):\n",
    "        dummy[str(i)][str(predicted[r])] +=1\n",
    "    \n",
    "    save_dict['conf_labels'] = [k for k in dummy.keys()]\n",
    "    # confusion matrix\n",
    "    conf_m = np.array([[j for j in i.values()] for i in dummy.values()]) # confusion matrix\n",
    "    save_dict['conf_matrix'] =  conf_m.tolist()\n",
    "    \n",
    "    #classwise \n",
    "    pre = np.sum(conf_m,axis=0)\n",
    "    rec = np.sum(conf_m,axis=1)\n",
    "    precision = [i[r] for r,i in enumerate(conf_m)]/pre \n",
    "    recall = [i[r] for r,i in enumerate(conf_m)]/rec \n",
    "    save_dict['classwise_prec_and_rec'] = [precision.tolist(), recall.tolist()]\n",
    "    \n",
    "    # micro precision and recall\n",
    "    precision = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(pre) \n",
    "    recall = sum([i[r] for r,i in enumerate(conf_m)])/np.sum(rec)\n",
    "    print(\"micro--\",precision, recall)\n",
    "    save_dict['micro_precision_and_recall'] = [precision.tolist(), recall.tolist()]\n",
    "    \n",
    "    # accuracy\n",
    "    acc = len([i for i in range(len(expected)) if expected[i]==predicted[i]])/len(expected)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    save_dict['accuracy'] = acc\n",
    "    \n",
    "    # micro f1 score\n",
    "    f_1 = 2*((precision*recall)/(precision+recall))\n",
    "    print(f\"F1 score: {f_1}\")\n",
    "    save_dict['micro_f1'] = f_1\n",
    "    \n",
    "    exp_results[fold] = save_dict\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "with open(\"exp_results.json\", \"w\")as f:\n",
    "    json.dump(exp_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exp_results.json', 'r') as f:\n",
    "    exp_results = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt = [[(j[1], pos_tags, t_tag, start_prob, transition_probability, emission_probability) for j in i] for i in val]\n",
    "\n",
    "def run(Viterbit, txt):\n",
    "    with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "        results = list(tqdm((executor.map(Viterbit, txt)), total=len(txt)))\n",
    "    return results\n",
    "\n",
    "print(\"starting the processes\")\n",
    "temp = run(Viterbit, txt)\n",
    "print(\"saving the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
